{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f98e30",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "# ICU survival modeling, Part 2: Cox and predictive models\n",
    "\n",
    "**Approach**\n",
    "- Teach and compare three models using the same preprocessing and the same train, validation, and test sets  \n",
    "  - Cox proportional hazards for time-to-event  \n",
    "  - Decision Tree classifier at fixed horizons  \n",
    "  - Random Forest classifier at fixed horizons\n",
    "\n",
    "**Learning goals**\n",
    "- Build fair, fixed-horizon predictions at 7, 30, and 60 days from ICU admission  \n",
    "- Understand discrimination vs calibration and why both matter clinically  \n",
    "- See interpretability trade-offs across Cox, a single tree, and an ensemble forest  \n",
    "- Practice leakage-free preprocessing with scikit-learn Pipelines\n",
    "\n",
    "**Data**\n",
    "- PhysioNet CinC Challenge 2012 ICU cohort, set A, 4000 stays, first 48 h features plus outcomes  \n",
    "- Outcomes available  \n",
    "  - Length of stay in days  \n",
    "  - Survival in days up to 2 years  \n",
    "  - In-hospital death indicator\n",
    "\n",
    "**Fair comparison plan**\n",
    "1) One reproducible split into train, validation, and test used by all models  \n",
    "2) Identical preprocessing via a single scikit-learn ColumnTransformer  \n",
    "3) Fixed-horizon evaluation at 7, 30, 60 days on the same evaluable patients per horizon  \n",
    "4) Report AUROC, area under precision-recall, Brier score, and compact calibration by bins\n",
    "\n",
    "**Clinical reading of metrics**\n",
    "- Discrimination ranks who is higher risk  \n",
    "- Calibration asks if predicted risk matches observed risk at a threshold relevant for action\n",
    "\n",
    "References  \n",
    "- Official challenge description and variable definitions, including Survival and In-hospital death  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc5b91",
   "metadata": {},
   "source": [
    "# 2. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Code cell — Setup and reproducibility\n",
    "\n",
    "# 2. Setup and reproducibility\n",
    "\n",
    "# Standard library\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "# Our helpers\n",
    "sys.path.append(\"/mnt/data\")\n",
    "import utils  # uses build_preprocessor, detect_feature_types, to_dataframe, labeling and calibration helpers\n",
    "from utils import (\n",
    "    fit_isotonic_calibrator, \n",
    "    apply_calibrator, \n",
    "    fixed_horizon_metrics, \n",
    "    align_evaluable, \n",
    "    select_threshold_by_net_benefit, \n",
    "    decision_curve_df\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Pandas and plotting defaults\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = Path(\"/mnt/data/PhysionetChallenge2012-set-a.csv.gz\")\n",
    "\n",
    "print(\"Versions\")\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"pandas\", pd.__version__)\n",
    "import sklearn\n",
    "print(\"scikit-learn\", sklearn.__version__)\n",
    "import lifelines\n",
    "print(\"lifelines\", lifelines.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be136e2e",
   "metadata": {},
   "source": [
    "# 3. Load data and define outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load data and define variables  [in-hospital death endpoint]\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "PATH = \"PhysionetChallenge2012-set-a.csv.gz\"\n",
    "\n",
    "# Simple check to ensure the data file exists before trying to load it\n",
    "if not os.path.exists(PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Error: The data file was not found at '{PATH}'. \"\n",
    "        \"Please ensure the dataset is in the correct directory.\"\n",
    "    )\n",
    "\n",
    "raw = pd.read_csv(PATH, compression=\"gzip\")\n",
    "    \n",
    "# Basic sanity checks\n",
    "required_cols = {\"Length_of_stay\", \"Survival\", \"In-hospital_death\"}\n",
    "missing = required_cols.difference(raw.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Endpoint locked to in-hospital death\n",
    "# Duration is time from ICU admission to discharge or in-hospital death\n",
    "# Event is 1 if died in hospital, 0 if discharged alive\n",
    "def make_outcomes_in_hospital(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    y = pd.DataFrame(index=df.index)\n",
    "    los = pd.to_numeric(df[\"Length_of_stay\"], errors=\"coerce\").astype(float)\n",
    "    los = np.clip(los, 0.0, None)  # clip negatives to 0 days\n",
    "    event = pd.to_numeric(df[\"In-hospital_death\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    y[\"duration_days\"] = los\n",
    "    y[\"event_death\"] = event\n",
    "    return y\n",
    "\n",
    "y = make_outcomes_in_hospital(raw)\n",
    "\n",
    "# Feature frame: drop outcome columns and obvious identifiers to avoid leakage\n",
    "drop_cols = [\"In-hospital_death\", \"Survival\", \"Length_of_stay\", \"recordid\"]\n",
    "X = raw.drop(columns=[c for c in drop_cols if c in raw.columns], errors=\"ignore\").copy()\n",
    "\n",
    "print(\"Outcome head\")\n",
    "display(y.head(3).style)\n",
    "print(\"Features shape\", X.shape)\n",
    "\n",
    "# Quick outcome summary\n",
    "event_rate = float(y[\"event_death\"].mean())\n",
    "duration = y[\"duration_days\"].to_numpy()\n",
    "iqr = float(np.percentile(duration, 75) - np.percentile(duration, 25))\n",
    "print(f\"Event rate: {event_rate:.3f}\")\n",
    "print(f\"Follow-up days: median {float(np.median(duration)):.1f}  IQR {iqr:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d590a",
   "metadata": {},
   "source": [
    "## 3.1 Preprocessing and leakage control\n",
    "- Detect numeric vs categorical features programmatically  \n",
    "- Numeric pipeline  \n",
    "  - SimpleImputer with median  \n",
    "- Categorical pipeline  \n",
    "  - SimpleImputer with most frequent  \n",
    "  - OneHotEncoder with ignore for unseen categories  \n",
    "- Build one ColumnTransformer used by all models inside scikit-learn Pipelines  \n",
    "- We exclude outcome variables and identifiers from the feature matrix to prevent target leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c63366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Build shared preprocessor\n",
    "\n",
    "num_cols, cat_cols = utils.detect_feature_types(X)\n",
    "print(\"Numeric\", len(num_cols), \"Categorical\", len(cat_cols))\n",
    "\n",
    "preprocessor: ColumnTransformer = utils.build_preprocessor(num_cols, cat_cols)\n",
    "\n",
    "# Fit preprocessor only on the training partition later; here we can preview on the full data safely without leaking,\n",
    "# but we will re-fit strictly on train after we split\n",
    "preprocessor.fit(X)\n",
    "Xt_preview = utils.to_dataframe(preprocessor, X.head(200))\n",
    "display(Xt_preview.head(3).style)\n",
    "print(\"Transformed preview shape\", Xt_preview.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702d98f",
   "metadata": {},
   "source": [
    "## 3.2. Splitting\n",
    "- Stratified split by the binary event to keep class balance stable  \n",
    "- Proportions  \n",
    "  - Train 60 percent  \n",
    "  - Validation 20 percent  \n",
    "  - Test 20 percent  \n",
    "- The same indices are reused for Cox, Decision Tree, and Random Forest  \n",
    "- We will always report metrics on the held-out test set and use validation only for light hyperparameter selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Create stratified train, validation, and test splits re-used across all models\n",
    "\n",
    "def make_splits(X: pd.DataFrame, y: pd.Series, seed: int = 42):\n",
    "    # First split off test 20 percent\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=seed)\n",
    "    train_val_idx, test_idx = next(sss1.split(X, y))\n",
    "    X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "    y_train_val, y_test = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Split train vs validation 75:25 within the remaining 80 percent to yield 60:20:20 overall\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    train_idx, val_idx = next(sss2.split(X_train_val, y_train_val))\n",
    "\n",
    "    idx_train = X_train_val.index[train_idx]\n",
    "    idx_val = X_train_val.index[val_idx]\n",
    "    idx_test = X_test.index\n",
    "\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "idx_train, idx_val, idx_test = make_splits(X, y[\"event_death\"], seed=SEED)\n",
    "\n",
    "print(\"Split sizes\",\n",
    "      \"train\", len(idx_train),\n",
    "      \"val\", len(idx_val),\n",
    "      \"test\", len(idx_test))\n",
    "\n",
    "# Materialize split datasets\n",
    "X_train, X_val, X_test = X.loc[idx_train], X.loc[idx_val], X.loc[idx_test]\n",
    "y_train, y_val, y_test = y.loc[idx_train], y.loc[idx_val], y.loc[idx_test]\n",
    "\n",
    "# Fit the shared preprocessor on train only\n",
    "preprocessor = utils.build_preprocessor(*utils.detect_feature_types(X_train))\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transformed DataFrames for convenience in Cox and for inspection\n",
    "Xt_train = utils.to_dataframe(preprocessor, X_train)\n",
    "Xt_val   = utils.to_dataframe(preprocessor, X_val)\n",
    "Xt_test  = utils.to_dataframe(preprocessor, X_test)\n",
    "\n",
    "print(\"Transformed shapes\",\n",
    "      Xt_train.shape, Xt_val.shape, Xt_test.shape)\n",
    "\n",
    "# Quick leakage sanity check: confirm no outcome columns survived\n",
    "assert not any(c.lower().startswith(\"in-hospital_death\") for c in Xt_train.columns)\n",
    "assert not any(c.lower().startswith(\"survival\") for c in Xt_train.columns)\n",
    "assert not any(c.lower().startswith(\"length_of_stay\") for c in Xt_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5b45c",
   "metadata": {},
   "source": [
    "# 4. Cox recap and fixed-horizon scoring\n",
    "\n",
    "What we keep: \n",
    "- Same endpoint in-hospital death with duration_days in days and event_death as the event  \n",
    "- Same preprocessing via the shared ColumnTransformer fit on train and applied to validation and test  \n",
    "- Same splits\n",
    "\n",
    "What we add now  \n",
    "- Fit a single multivariable Cox model on the preprocessed training set  \n",
    "- Compute fixed-horizon risks at 7, 30, and 60 days on validation and test  \n",
    "- Evaluate discrimination, calibration, and overall accuracy on the identical evaluable cohorts per horizon  \n",
    "- Store predictions, evaluability masks, and a tidy metrics table for later comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Cox recap and fixed-horizon scoring\n",
    "\n",
    "# Horizons in days\n",
    "HORIZONS = [7, 30, 60]\n",
    "\n",
    "# 1) Prepare modeling frames for lifelines using preprocessed matrices from earlier steps\n",
    "df_train = Xt_train.copy()\n",
    "df_train[\"duration_days\"] = y_train[\"duration_days\"].values\n",
    "df_train[\"event_death\"]   = y_train[\"event_death\"].values\n",
    "\n",
    "df_val = Xt_val.copy()\n",
    "df_val[\"duration_days\"] = y_val[\"duration_days\"].values\n",
    "df_val[\"event_death\"]   = y_val[\"event_death\"].values\n",
    "\n",
    "df_test = Xt_test.copy()\n",
    "df_test[\"duration_days\"] = y_test[\"duration_days\"].values\n",
    "df_test[\"event_death\"]   = y_test[\"event_death\"].values\n",
    "\n",
    "# 2) Fit Cox on train only\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_train, duration_col=\"duration_days\", event_col=\"event_death\")\n",
    "\n",
    "# 3) Cox fixed-horizon risks using utils, no renaming\n",
    "cph_pred_train = utils.predict_fixed_horizon_risk_from_cox(cph, Xt_train, [7, 30, 60])\n",
    "cph_pred_val   = utils.predict_fixed_horizon_risk_from_cox(cph, Xt_val,   [7, 30, 60])\n",
    "cph_pred_test  = utils.predict_fixed_horizon_risk_from_cox(cph, Xt_test,  [7, 30, 60])\n",
    "\n",
    "# 4) Fixed-horizon labels and evaluability masks using utils signature\n",
    "labels_train, labels_val, labels_test = {}, {}, {}\n",
    "for h in [7, 30, 60]:\n",
    "    yb_tr, m_tr = utils.get_fixed_horizon_labels(y_train, horizon_days=h)\n",
    "    yb_va, m_va = utils.get_fixed_horizon_labels(y_val,   horizon_days=h)\n",
    "    yb_te, m_te = utils.get_fixed_horizon_labels(y_test,  horizon_days=h)\n",
    "    labels_train[h] = {\"y_true\": pd.Series(yb_tr, index=y_train.index).astype(int),\n",
    "                       \"mask\":   pd.Series(m_tr,  index=y_train.index).astype(bool)}\n",
    "    labels_val[h]   = {\"y_true\": pd.Series(yb_va, index=y_val.index).astype(int),\n",
    "                       \"mask\":   pd.Series(m_va, index=y_val.index).astype(bool)}\n",
    "    labels_test[h]  = {\"y_true\": pd.Series(yb_te, index=y_test.index).astype(int),\n",
    "                       \"mask\":   pd.Series(m_te, index=y_test.index).astype(bool)}\n",
    "\n",
    "# 5) Metrics per horizon on identical evaluable cohorts\n",
    "records = []\n",
    "for set_name, preds, labels in [\n",
    "    (\"train\", cph_pred_train, labels_train),\n",
    "    (\"val\",   cph_pred_val,   labels_val),\n",
    "    (\"test\",  cph_pred_test,  labels_test),\n",
    "]:\n",
    "    for h in [7, 30, 60]:\n",
    "        col = f\"Risk_{h}d\"              # use utils naming\n",
    "        m = labels[h][\"mask\"]\n",
    "        y = labels[h][\"y_true\"][m].to_numpy()\n",
    "        p = preds[col][m].to_numpy()\n",
    "        if y.size == 0 or len(np.unique(y)) < 2:\n",
    "            auroc = np.nan; auprc = np.nan; brier = np.nan\n",
    "        else:\n",
    "            auroc = roc_auc_score(y, p)\n",
    "            auprc = average_precision_score(y, p)\n",
    "            brier = brier_score_loss(y, p)\n",
    "        records.append({\n",
    "            \"model\": \"Cox\",\n",
    "            \"set\": set_name,\n",
    "            \"horizon_days\": h,\n",
    "            \"auroc\": float(auroc) if np.isfinite(auroc) else np.nan,\n",
    "            \"auprc\": float(auprc) if np.isfinite(auprc) else np.nan,\n",
    "            \"brier\": float(brier) if np.isfinite(brier) else np.nan,\n",
    "            \"n_evaluable\": int(m.sum())\n",
    "        })\n",
    "\n",
    "cph_metrics = pd.DataFrame.from_records(records).sort_values([\"set\", \"horizon_days\"])\n",
    "display(cph_metrics.style)\n",
    "\n",
    "# Optional compact calibration summaries on test using sklearn-style bins\n",
    "calib_cph_test = {}\n",
    "for h in [7, 30, 60]:\n",
    "    m = labels_test[h][\"mask\"]\n",
    "    y = labels_test[h][\"y_true\"][m].to_numpy()\n",
    "    p = cph_pred_test[f\"Risk_{h}d\"][m].to_numpy()\n",
    "    if y.size == 0 or len(np.unique(y)) < 2:\n",
    "        calib_cph_test[h] = pd.DataFrame({\"prob_mean\": [], \"event_rate\": []})\n",
    "    else:\n",
    "        frac_pos, prob_mean = calibration_curve(y, p, n_bins=10, strategy=\"quantile\")\n",
    "        calib_cph_test[h] = pd.DataFrame({\"prob_mean\": prob_mean, \"event_rate\": frac_pos})\n",
    "\n",
    "# Store artifacts unchanged\n",
    "ARTIFACTS = {\n",
    "    \"cph_model\": cph,\n",
    "    \"cph_pred_train\": cph_pred_train,\n",
    "    \"cph_pred_val\":   cph_pred_val,\n",
    "    \"cph_pred_test\":  cph_pred_test,\n",
    "    \"labels_train\": labels_train,\n",
    "    \"labels_val\":   labels_val,\n",
    "    \"labels_test\":  labels_test,\n",
    "    \"cph_metrics\":  cph_metrics,\n",
    "    \"calib_cph_test\": calib_cph_test,\n",
    "    \"horizons_days\": [7, 30, 60],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ea472",
   "metadata": {},
   "source": [
    "#### **Quick recap on metrics**\n",
    "* **`auroc`** (Area Under the ROC Curve): This is a measure of **ranking**. It tells us if the model is good at giving higher-risk patients higher scores than lower-risk patients\n",
    "* **`auprc`** (Area Under the Precision-Recall Curve): This is a measure of **positive prediction value**, which is very useful when the event (death) is rare. It answers: \"When the model predicts a patient is high-risk, how often is it correct?\"\n",
    "* **`brier`**: This measures the **accuracy of the probability score itself**. It penalizes models for being overconfident or underconfident. A perfect score is 0. **Lower is better**\n",
    "\n",
    "**Clinical takeaways**\n",
    "* **It's a strong baseline**: An AUROC of ~0.80 on the test set for short-term risk is a solid result and provides genuine clinical value. It shows that the model is effective at identifying which patients are at higher risk\n",
    "* **Actionability**: Before using these risk scores to make decisions (e.g., flagging a patient for a clinical review), we would also need to check its **calibration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01527f8",
   "metadata": {},
   "source": [
    "# 5. Decision Tree setup and training\n",
    "### Decision Tree at fixed horizons\n",
    "\n",
    "- While the Cox model looks at the *rate* of events over time, a Decision Tree works differently. It learns a series of simple \"if-then\" rules from the data to predict a binary outcome, like \"will this patient die within 7 days?\"\n",
    "\n",
    "**Why this model**\n",
    "* **High Interpretability**: A small Decision Tree is like a flowchart. We can literally draw it and follow the logic, making it very transparent and easy to explain to clinicians.\n",
    "* **No Proportional Hazards Assumption**: Unlike the Cox model, it doesn't assume a variable's effect is constant over time. It can learn different rules for different patient subgroups.\n",
    "* **Captures Interactions**: It can naturally find patterns like \"if `SOFA` score is high AND the patient is on a ventilator, then the risk is very high.\"\n",
    "\n",
    "**Evaluation**\n",
    "- Train on the evaluable training cohort for each horizon\n",
    "- Use light cross-validation on the training set to choose max_depth and min_samples_leaf\n",
    "- Score on validation and test with the same evaluable cohorts used for Cox\n",
    "- Store predictions and metrics for side-by-side comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Decision Tree at fixed horizons\n",
    "\n",
    "# Use the shared horizons and labels from ARTIFACTS\n",
    "HORIZONS = ARTIFACTS[\"horizons_days\"]\n",
    "labels_train = ARTIFACTS[\"labels_train\"]\n",
    "labels_val   = ARTIFACTS[\"labels_val\"]\n",
    "labels_test  = ARTIFACTS[\"labels_test\"]\n",
    "\n",
    "# We wrap our already-fitted preprocessor in a FunctionTransformer.\n",
    "# This prevents the pipeline from trying to re-fit it, which would cause data leakage.\n",
    "# All models MUST use the exact same preprocessing rules learned only from the original training set.\n",
    "frozen_pre = FunctionTransformer(lambda X: preprocessor.transform(X))\n",
    "\n",
    "# Define a simple pipeline and a small grid for hyperparameter tuning.\n",
    "pipe_tree = Pipeline(steps=[\n",
    "    (\"pre\", frozen_pre),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=SEED, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# These are the \"knobs\" we'll turn to find the best, simplest tree.\n",
    "param_grid = {\n",
    "    \"clf__max_depth\": [3, 4, 5, 6],\n",
    "    \"clf__min_samples_leaf\": [25, 50, 100]\n",
    "}\n",
    "\n",
    "# Containers for predictions and metrics\n",
    "dt_pred_train = pd.DataFrame(index=X_train.index)\n",
    "dt_pred_val   = pd.DataFrame(index=X_val.index)\n",
    "dt_pred_test  = pd.DataFrame(index=X_test.index)\n",
    "rows = []\n",
    "\n",
    "print(\"Training Decision Tree for each horizon...\")\n",
    "for h in HORIZONS:\n",
    "    # Get the correct labels and masks for this specific horizon\n",
    "    m_tr = labels_train[h][\"mask\"]\n",
    "    m_va = labels_val[h][\"mask\"]\n",
    "    m_te = labels_test[h][\"mask\"]\n",
    "\n",
    "    y_tr = labels_train[h][\"y_true\"][m_tr].to_numpy()\n",
    "    y_va = labels_val[h][\"y_true\"][m_va].to_numpy()\n",
    "    y_te = labels_test[h][\"y_true\"][m_te].to_numpy()\n",
    "\n",
    "    # We fit the grid search ONLY on the patients who are \"evaluable\" for this horizon.\n",
    "    # This ensures the model learns from the most relevant data.\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe_tree,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\", # We'll judge the best tree based on its ranking ability\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train.loc[m_tr], y_tr)\n",
    "\n",
    "    # Store a compact view of the chosen hyperparameters\n",
    "    best_params = grid.best_params_\n",
    "    print(f\"Horizon {h}d best params: {best_params}\")\n",
    "\n",
    "    # Predict probabilities for ALL patients using the best model found\n",
    "    p_tr = grid.predict_proba(X_train)[:, 1]\n",
    "    p_va = grid.predict_proba(X_val)[:, 1]\n",
    "    p_te = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    colname = f\"DT_Risk_{h}d\"\n",
    "    dt_pred_train[colname] = p_tr\n",
    "    dt_pred_val[colname]   = p_va\n",
    "    dt_pred_test[colname]  = p_te\n",
    "\n",
    "    # Helper to calculate metrics safely on the evaluable cohorts\n",
    "    def _safe_metrics(y, p):\n",
    "        if y.size == 0 or len(np.unique(y)) < 2:\n",
    "            return np.nan, np.nan, np.nan\n",
    "        return roc_auc_score(y, p), average_precision_score(y, p), brier_score_loss(y, p)\n",
    "\n",
    "    au_tr, ap_tr, br_tr = _safe_metrics(y_tr, p_tr[m_tr])\n",
    "    au_va, ap_va, br_va = _safe_metrics(y_va, p_va[m_va])\n",
    "    au_te, ap_te, br_te = _safe_metrics(y_te, p_te[m_te])\n",
    "\n",
    "    rows += [\n",
    "        {\"model\": \"DecisionTree\", \"set\": \"train\", \"horizon_days\": h,\n",
    "         \"auroc\": au_tr, \"auprc\": ap_tr, \"brier\": br_tr, \"n_evaluable\": int(m_tr.sum()),\n",
    "         \"best_max_depth\": best_params[\"clf__max_depth\"], \"best_min_samples_leaf\": best_params[\"clf__min_samples_leaf\"]},\n",
    "        {\"model\": \"DecisionTree\", \"set\": \"val\",   \"horizon_days\": h,\n",
    "         \"auroc\": au_va, \"auprc\": ap_va, \"brier\": br_va, \"n_evaluable\": int(m_va.sum()),\n",
    "         \"best_max_depth\": best_params[\"clf__max_depth\"], \"best_min_samples_leaf\": best_params[\"clf__min_samples_leaf\"]},\n",
    "        {\"model\": \"DecisionTree\", \"set\": \"test\",  \"horizon_days\": h,\n",
    "         \"auroc\": au_te, \"auprc\": ap_te, \"brier\": br_te, \"n_evaluable\": int(m_te.sum()),\n",
    "         \"best_max_depth\": best_params[\"clf__max_depth\"], \"best_min_samples_leaf\": best_params[\"clf__min_samples_leaf\"]},\n",
    "    ]\n",
    "\n",
    "# Tidy metrics table for the tree\n",
    "dt_metrics = pd.DataFrame(rows).sort_values([\"set\", \"horizon_days\"]).reset_index(drop=True)\n",
    "\n",
    "# Round for readability\n",
    "for c in [\"auroc\", \"auprc\", \"brier\"]:\n",
    "    dt_metrics[c] = pd.to_numeric(dt_metrics[c], errors=\"coerce\").round(3)\n",
    "\n",
    "order_sets = pd.CategoricalDtype([\"train\", \"val\", \"test\"], ordered=True)\n",
    "dt_metrics[\"set\"] = dt_metrics[\"set\"].astype(order_sets)\n",
    "\n",
    "display(dt_metrics.style)\n",
    "\n",
    "# Persist into ARTIFACTS for later side-by-side plots and tables\n",
    "ARTIFACTS[\"dt_model_cv\"] = \"per-horizon GridSearchCV objects not stored to keep memory light\"\n",
    "ARTIFACTS[\"dt_pred_train\"] = dt_pred_train\n",
    "ARTIFACTS[\"dt_pred_val\"]   = dt_pred_val\n",
    "ARTIFACTS[\"dt_pred_test\"]  = dt_pred_test\n",
    "ARTIFACTS[\"dt_metrics\"]    = dt_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c14f05",
   "metadata": {},
   "source": [
    "#### **Clinical Insights**\n",
    "\n",
    "* **Ranking Ability (AUROC)**\n",
    "    - The tree is reasonably good at identifying higher-risk patients, with a test AUROC ≈ 0.68–0.76 at the 7-day mark.  but not as smoothly as the Cox model\n",
    "    - This highlights a classic machine learning trade-off: we've gained a simple, flowchart-like model but have lost some predictive precision\n",
    "* **Overfitting**\n",
    "    - Notice the performance gap between the `train` set and the `test` set\n",
    "* **\"Tuning\" the Tree's Complexity**\n",
    "    - The `best_max_depth` and `best_min_samples_leaf` columns show the results of our automatic tuning. By keeping the tree \"shallow\" (only 3-4 levels of questions), we ensure the final model is easy to read and understand, which is its main advantage\n",
    "* **The Power of Transparency**\n",
    "    - This model is best suited for generating simple, understandable prognostic rules for quick bedside evaluation. It is less suited for applications requiring the highest possible accuracy in risk scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345ba8d",
   "metadata": {},
   "source": [
    "## 5.1. **Looking Inside the Box**\n",
    "\n",
    "The main advantage of a Decision Tree is its transparency. To do this, we will focus on the model trained to predict risk at the **7-day horizon**\n",
    "1.  **Visualize the Tree:** We will create a flowchart of the model to see the exact \"if-then\" rules it learned from the data\n",
    "2.  **Identify Key Features:** We'll rank the clinical variables by importance to see which ones the tree relied on most heavily\n",
    "3.  **Audit the Rules on Test Data:** We will check if the groups of patients identified by the tree's rules have high and low death rates in the unseen test set, confirming the model's logic holds up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdaf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Re-fit a single, clean tree for the 7-day horizon ---\n",
    "# We use the best settings (hyperparameters) we already found via GridSearchCV.\n",
    "\n",
    "HORIZON = 7 # Focus on the 7-day model\n",
    "\n",
    "# Get the best parameters from our previous results\n",
    "best_params_row = ARTIFACTS[\"dt_metrics\"].query(\"set == 'val' and horizon_days == @HORIZON\").iloc[0]\n",
    "best_depth = int(best_params_row[\"best_max_depth\"])\n",
    "best_leaf_samples = int(best_params_row[\"best_min_samples_leaf\"])\n",
    "\n",
    "print(f\"Refitting 7-day tree with max_depth={best_depth} and min_samples_leaf={best_leaf_samples}\\\\n\")\n",
    "\n",
    "# Get the correct \"evaluable\" data for this horizon\n",
    "mask_train = ARTIFACTS[\"labels_train\"][HORIZON][\"mask\"]\n",
    "y_train_h = ARTIFACTS[\"labels_train\"][HORIZON][\"y_true\"][mask_train].to_numpy()\n",
    "\n",
    "# Use the already preprocessed data\n",
    "Xt_train_h = Xt_train.loc[mask_train]\n",
    "\n",
    "# Fit the final Decision Tree model\n",
    "final_tree = DecisionTreeClassifier(\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced\", # Helps the model pay attention to the rare \"death\" event\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_leaf_samples\n",
    ")\n",
    "final_tree.fit(Xt_train_h, y_train_h)\n",
    "\n",
    "# --- Step 2: Plot the tree to visualize the rules ---\n",
    "plt.figure(figsize=(18, 10))\n",
    "plot_tree(\n",
    "    final_tree,\n",
    "    feature_names=Xt_train_h.columns.tolist(),\n",
    "    class_names=[\"Survived\", \"Died\"], # Label for clarity\n",
    "    filled=True,       # Color nodes by majority class\n",
    "    impurity=False,    # Hide Gini impurity for a cleaner look\n",
    "    proportion=True,   # Show percentage of samples in each class\n",
    "    rounded=True,      # Use rounded boxes\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(f\"Decision Tree Rules for {HORIZON}-Day In-Hospital Death\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# --- Step 3: Export the same rules as text ---\n",
    "# This can be useful for documentation.\n",
    "rules_text = export_text(final_tree, feature_names=list(Xt_train_h.columns))\n",
    "print(\"--- Decision Rules as Text ---\\\\n\")\n",
    "print(rules_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105fee5",
   "metadata": {},
   "source": [
    "**Potential/ Hypotherical clinical implications**\n",
    "\n",
    "**High-risk neuro flag when recent GCS is low**\n",
    "- Trigger if GCS_last ≤ 7.5, especially with HCT_first ≤ 31.65 or GCS_highest ≤ 9.5\n",
    "    - e.g. Action 1 h senior review, tighter neuro checks, early airway readiness, check anemia and oxygen delivery, investigate reversible causes\n",
    "\n",
    "**Kidney-stress alert when early BUN is high despite acceptable GCS**\n",
    "- Trigger if GCS_last > 7.5 and BUN_first > 24.5\n",
    "    - e.g. Action activate acute kidney injury prevention bundle strict input–output, review nephrotoxic drugs, optimize fluids, targeted labs, nephrology consult if rising trend\n",
    "\n",
    "**Ventilation-escalation huddle when ventilation is prolonged with renal signal**\n",
    "- Trigger if BUN_first > 24.5 and MechVentDuration > 1786 in the dataset’s units\n",
    "    - e.g. Action daily extubation-readiness protocol, ventilator-associated pneumonia prevention checks, sedation and mobility optimization, multidisciplinary huddle to reduce time on ventilator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Feature Importance (which variables did the tree use?) ---\n",
    "\n",
    "# The default 'Gini importance' measures how much a feature helps to create \"pure\" nodes.\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"feature\": Xt_train_h.columns,\n",
    "    \"importance\": final_tree.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False).head(15)\n",
    "\n",
    "print(\"--- Top 15 Features (Gini Importance) ---\")\n",
    "display(feature_importances.style)\n",
    "\n",
    "\n",
    "# --- Step 5: Auditing the tree's rules (leaves) on the test set ---\n",
    "# This is the most critical step: do the rules that predict high risk actually\n",
    "# correspond to groups of patients with high death rates in the unseen test data?\n",
    "\n",
    "# Get the correct \"evaluable\" test data for this horizon\n",
    "mask_test = ARTIFACTS[\"labels_test\"][HORIZON][\"mask\"]\n",
    "y_test_h = ARTIFACTS[\"labels_test\"][HORIZON][\"y_true\"][mask_test].to_numpy()\n",
    "Xt_test_h = Xt_test.loc[mask_test]\n",
    "\n",
    "# Find out which leaf each test patient ends up in\n",
    "leaf_ids_test = final_tree.apply(Xt_test_h)\n",
    "# Get the predicted probability for each test patient\n",
    "probabilities_test = final_tree.predict_proba(Xt_test_h)[:, 1]\n",
    "\n",
    "# Create a summary table\n",
    "leaf_audit_df = pd.DataFrame({\n",
    "    \"leaf_id\": leaf_ids_test,\n",
    "    \"true_outcome\": y_test_h,\n",
    "    \"predicted_risk\": probabilities_test\n",
    "})\n",
    "\n",
    "leaf_summary = (\n",
    "    leaf_audit_df\n",
    "    .groupby(\"leaf_id\")\n",
    "    .agg(\n",
    "        num_patients=(\"true_outcome\", \"size\"),\n",
    "        observed_death_rate=(\"true_outcome\", \"mean\"),\n",
    "        avg_predicted_risk=(\"predicted_risk\", \"mean\")\n",
    "    )\n",
    "    .sort_values(\"avg_predicted_risk\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\\\n--- Leaf Performance Audit on Test Set ---\")\n",
    "display(leaf_summary.style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence for features actually used by the tree at 7 days\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "H = 7  # horizon to visualize\n",
    "\n",
    "# 1) Recreate the evaluable training slice and labels\n",
    "m_tr = ARTIFACTS[\"labels_train\"][H][\"mask\"]\n",
    "Xtr_t = Xt_train.loc[m_tr]\n",
    "y_tr  = ARTIFACTS[\"labels_train\"][H][\"y_true\"][m_tr].to_numpy()\n",
    "\n",
    "# 2) Pull the best hyperparameters we selected for this horizon\n",
    "best_row  = ARTIFACTS[\"dt_metrics\"].query(\"set == 'val' and horizon_days == @H\").iloc[0]\n",
    "best_depth = int(best_row[\"best_max_depth\"])\n",
    "best_leaf  = int(best_row[\"best_min_samples_leaf\"])\n",
    "\n",
    "# 3) Fit the shallow tree on the same transformed data\n",
    "dt = DecisionTreeClassifier(\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced\",\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_leaf\n",
    ").fit(Xtr_t, y_tr)\n",
    "\n",
    "# 4) Pick features the tree actually used (nonzero importance) and prefer continuous ones\n",
    "imp = pd.Series(dt.feature_importances_, index=Xtr_t.columns)\n",
    "used = imp[imp > 0].sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Heuristic: continuous features created by the preprocessor often start with \"num__\"\n",
    "cont_used = [f for f in used if f.startswith(\"num__\")]\n",
    "targets = cont_used[:3] if len(cont_used) > 0 else used[:3]\n",
    "\n",
    "if len(targets) == 0:\n",
    "    print(\"No features with nonzero importance were found for PDP\")\n",
    "else:\n",
    "    print(\"Plotting PDP for:\", targets)\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        dt, Xtr_t, targets, kind=\"average\", grid_resolution=50\n",
    "    )\n",
    "    plt.suptitle(f\"Partial Dependence of Predicted Risk at {H}-Day Horizon\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c33cc8",
   "metadata": {},
   "source": [
    "#### **Insights from Partial Dependence Plots**\n",
    "\n",
    "These plots isolate how the model's risk prediction changes based on a single feature, showing the rules it learned graphically.\n",
    "\n",
    "* **Plots Show Step-Functions**: Each jump is a specific if-then split the tree learned. The flat lines mean all patients in that range are treated identically by that rule\n",
    "* **Glasgow Coma Scale (GCS)**: The model learned that lower GCS scores are linked to higher mortality risk. The risk drops sharply once a patient's GCS score is above a certain threshold\n",
    "* **Blood Urea Nitrogen (BUN)**: An elevated initial BUN measurement acts as a risk threshold. When BUN crosses a specific value, the predicted risk jumps, correctly identifying kidney function's role in patient stability\n",
    "\n",
    "**Key Takeaways**\n",
    "* **Clinically Valid Rules**: The tree learns simple, communicable rules that align with clinical intuition (e.g., low consciousness or poor kidney function increases risk)\n",
    "* **Strengths and Limits**: These plots are excellent for explaining the model's logic for a single variable. However, they show an averaged effect and are not causal. The rules are coarse and may miss more gradual trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ea933",
   "metadata": {},
   "source": [
    "# 6. Random Forest at Fixed Horizons\n",
    "\n",
    "If a single Decision Tree is like asking one expert for their opinion, a Random Forest is like asking a large, diverse committee of hundreds of experts and taking their majority vote\n",
    "\n",
    "* **The \"Wisdom of the Crowd\"**: It's an \"ensemble\" of many individual Decision Trees. Each tree is trained on a slightly different random sample of the data and is only allowed to consider a random subset of features for each split\n",
    "* **Reduces Overfitting**: This randomness prevents any single tree from becoming too specialized or \"memorizing\" the training data. The errors made by one tree are often canceled out by the others, leading to a more stable and accurate model that generalizes better to new patients\n",
    "* **High Predictive Power**: Random Forests are one of the most powerful \"out-of-the-box\" classifiers. They often achieve high performance without extensive tuning\n",
    "\n",
    "**The Trade-Off**\n",
    "The cost of this increased power is a loss of direct interpretability. We can no longer draw a single flowchart of the rules. However, we can still measure which features were most important to the \"committee\" as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d36911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizons and labels from our central ARTIFACTS store\n",
    "HORIZONS = ARTIFACTS[\"horizons_days\"]\n",
    "labels_train = ARTIFACTS[\"labels_train\"]\n",
    "labels_val   = ARTIFACTS[\"labels_val\"]\n",
    "labels_test  = ARTIFACTS[\"labels_test\"]\n",
    "\n",
    "# Use the same frozen preprocessor to prevent data leakage\n",
    "frozen_pre = FunctionTransformer(lambda X: preprocessor.transform(X))\n",
    "\n",
    "# --- Define the Pipeline and Search Grid ---\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    (\"pre\", frozen_pre),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        random_state=SEED,\n",
    "        # 'balanced_subsample' adjusts weights for the minority class in each bootstrap sample.\n",
    "        # This is a robust way to handle the low death rate in our data.\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1 # Use all available CPU cores for faster training\n",
    "    ))\n",
    "])\n",
    "\n",
    "# A small, practical grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"clf__n_estimators\": [100, 200],      # How many trees in the forest\n",
    "    \"clf__max_depth\": [4, 6, 8],          # Max depth of each tree\n",
    "    \"clf__min_samples_leaf\": [25, 50]   # Min patients in a final leaf\n",
    "}\n",
    "\n",
    "# --- Containers for predictions and metrics ---\n",
    "rf_pred_train = pd.DataFrame(index=X_train.index)\n",
    "rf_pred_val   = pd.DataFrame(index=X_val.index)\n",
    "rf_pred_test  = pd.DataFrame(index=X_test.index)\n",
    "records = []\n",
    "\n",
    "print(\"Training Random Forest for each horizon...\")\n",
    "for h in HORIZONS:\n",
    "    # Get the correct evaluable cohorts for this horizon\n",
    "    m_tr = labels_train[h][\"mask\"]\n",
    "    m_va = labels_val[h][\"mask\"]\n",
    "    m_te = labels_test[h][\"mask\"]\n",
    "\n",
    "    y_tr = labels_train[h][\"y_true\"][m_tr].to_numpy()\n",
    "    y_va = labels_val[h][\"y_true\"][m_va].to_numpy()\n",
    "    y_te = labels_test[h][\"y_true\"][m_te].to_numpy()\n",
    "\n",
    "    # Fit GridSearchCV only on the evaluable training data for this horizon\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe_rf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train.loc[m_tr], y_tr)\n",
    "\n",
    "    best_params = grid.best_params_\n",
    "    print(f\"Horizon {h}d best params: {best_params}\")\n",
    "\n",
    "    # Get predictions from the best model\n",
    "    p_tr = grid.predict_proba(X_train)[:, 1]\n",
    "    p_va = grid.predict_proba(X_val)[:, 1]\n",
    "    p_te = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Store predictions\n",
    "    colname = f\"RF_Risk_{h}d\"\n",
    "    rf_pred_train[colname] = p_tr\n",
    "    rf_pred_val[colname]   = p_va\n",
    "    rf_pred_test[colname]  = p_te\n",
    "\n",
    "    # Calculate metrics on the identical evaluable cohorts\n",
    "    def _safe_metrics(y, p):\n",
    "        if y.size == 0 or len(np.unique(y)) < 2:\n",
    "            return np.nan, np.nan, np.nan\n",
    "        return roc_auc_score(y, p), average_precision_score(y, p), brier_score_loss(y, p)\n",
    "\n",
    "    au_tr, ap_tr, br_tr = _safe_metrics(y_tr, p_tr[m_tr])\n",
    "    au_va, ap_va, br_va = _safe_metrics(y_va, p_va[m_va])\n",
    "    au_te, ap_te, br_te = _safe_metrics(y_te, p_te[m_te])\n",
    "\n",
    "    # Append results for all sets to our records list\n",
    "    for s, au, ap, br, mask in [\n",
    "        (\"train\", au_tr, ap_tr, br_tr, m_tr),\n",
    "        (\"val\",   au_va, ap_va, br_va, m_va),\n",
    "        (\"test\",  au_te, ap_te, br_te, m_te),\n",
    "    ]:\n",
    "        records.append({\n",
    "            \"model\": \"RandomForest\",\n",
    "            \"set\": s,\n",
    "            \"horizon_days\": h,\n",
    "            \"auroc\": float(au),\n",
    "            \"auprc\": float(ap),\n",
    "            \"brier\": float(br),\n",
    "            \"n_evaluable\": int(mask.sum()),\n",
    "            \"best_n_estimators\": best_params[\"clf__n_estimators\"],\n",
    "            \"best_max_depth\": best_params[\"clf__max_depth\"],\n",
    "            \"best_min_samples_leaf\": best_params[\"clf__min_samples_leaf\"]\n",
    "        })\n",
    "\n",
    "# --- Create and display the final metrics table ---\n",
    "rf_metrics = pd.DataFrame.from_records(records).sort_values([\"set\", \"horizon_days\"]).reset_index(drop=True)\n",
    "for c in [\"auroc\", \"auprc\", \"brier\"]:\n",
    "    rf_metrics[c] = pd.to_numeric(rf_metrics[c], errors=\"coerce\").round(3)\n",
    "\n",
    "order_sets = pd.CategoricalDtype([\"train\", \"val\", \"test\"], ordered=True)\n",
    "rf_metrics[\"set\"] = rf_metrics[\"set\"].astype(order_sets)\n",
    "\n",
    "display(rf_metrics.style)\n",
    "\n",
    "# --- Store results for our final comparison ---\n",
    "ARTIFACTS[\"rf_pred_train\"] = rf_pred_train\n",
    "ARTIFACTS[\"rf_pred_val\"]   = rf_pred_val\n",
    "ARTIFACTS[\"rf_pred_test\"]  = rf_pred_test\n",
    "ARTIFACTS[\"rf_metrics\"]    = rf_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601917b",
   "metadata": {},
   "source": [
    "### **Clinical Insights**\n",
    "\n",
    "* **Highest Predictive Accuracy**\n",
    "    The Random Forest achieves the best discrimination on the test set, with an **AUROC of 0.82** at the 7-day horizon. This is a clear improvement over both the Cox model (~0.80) and the single Decision Tree (~0.76), making it the most effective model for ranking patients by risk\n",
    "\n",
    "* **Better Generalization**\n",
    "    While there is still a gap between training and test scores, the ensemble approach makes the Random Forest more robust than a single tree\n",
    "\n",
    "* **The Interpretability Trade-Off**\n",
    "    This boost in accuracy comes at a cost: we can no longer visualize a simple set of if-then rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2522a0",
   "metadata": {},
   "source": [
    "## 6.1. But how useful is this RF model?\n",
    "\n",
    "Our Random Forest model is the most accurate predictor, but accuracy alone isn't enough. We need to verify it works in a way that is trustworthy, fair, and practical for clinical use\n",
    "\n",
    "* **1. Feature Importance**\n",
    "    * Ranks the clinical variables the model found most predictive.\n",
    "    * uilds trust by confirming the model uses clinically sensible factors (e.g., SOFA score) to make decisions.\n",
    "\n",
    "* **2. Risk Stratification**\n",
    "    * Checks if patients the model calls \"high-risk\" actually have a higher death rate in the real data.\n",
    "    * Verifies that a high predicted risk score from the model corresponds to a genuinely high-risk patient.\n",
    "\n",
    "* **3. Decision Threshold Analysis**\n",
    "    * Simulates a clinical alert (e.g., \"flag patients with >20% risk\") to see the trade-off between correct alerts and false alarms.\n",
    "    * Helps determine a practical action threshold for using the model without causing \"alert fatigue\" for clinicians\n",
    "\n",
    "* **4. Subgroup Fairness Check**\n",
    "    * Tests if the model is equally accurate for different patient groups (e.g., across ICU types).\n",
    "    * An ethical check to ensure the model is fair and doesn't have a hidden bias against a specific patient population.\n",
    "\n",
    "* **5. Decision Curve Analysis (DCA)**\n",
    "    * A plot to weigh the benefits of the model's predictions against the harm of unnecessary interventions.\n",
    "    * Answers the critical question: \"Is using this model better than simply treating all patients, or treating none?\" It helps quantify the model's real-world value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b20c2",
   "metadata": {},
   "source": [
    "#### Analysis 1: Feature Importance\n",
    "\n",
    "This is our first step in \"opening the black box\". We will ask the Random Forest which clinical variables it found most useful for making its predictions. We use two methods to ensure our results are reliable.\n",
    "\n",
    "* **What this does**:\n",
    "    * **Impurity Importance**: Ranks features by how much they help create \"pure\" groups of patients (i.e., groups that are all survivors or all deaths). It's fast but can sometimes be biased.\n",
    "    * **Permutation Importance**: Ranks features by shuffling their values and measuring how much this shuffle hurts the model's performance. It is more computationally intensive but often more reliable.\n",
    "* **What to look for**:\n",
    "    * We want to see clinically sensible variables (like SOFA, GCS, BUN) at the top of both lists.\n",
    "    * Strong agreement between the two methods gives us confidence that the model has identified stable, meaningful patterns\n",
    "    * If a feature is high in both plots, it is a stable, influential driver\n",
    "    * If a feature is high in impurity and low in permutation, it may be a proxy or splitting convenience rather than a true predictor\n",
    "    * If a feature is modest in impurity but strong in permutation, it may interact with others in a way the forest captures beyond single split gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 6 features by impurity importance as a bar chart\n",
    "\n",
    "# Use the fitted random forest for the 7-day horizon from the grid search\n",
    "# This is the best estimator from the grid search for H=7\n",
    "H = 7\n",
    "m_tr = ARTIFACTS[\"labels_train\"][H][\"mask\"]\n",
    "Xtr_t = Xt_train.loc[m_tr]\n",
    "y_tr = ARTIFACTS[\"labels_train\"][H][\"y_true\"][m_tr].to_numpy()\n",
    "\n",
    "# Refit the random forest for the 7-day horizon using the best hyperparameters\n",
    "best_row = ARTIFACTS[\"rf_metrics\"].query(\"set == 'test' and horizon_days == @H\").iloc[0]\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_estimators=int(best_row[\"best_n_estimators\"]),\n",
    "    max_depth=int(best_row[\"best_max_depth\"]),\n",
    "    min_samples_leaf=int(best_row[\"best_min_samples_leaf\"]),\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(Xtr_t, y_tr)\n",
    "\n",
    "imp_series = pd.Series(rf.feature_importances_, index=Xtr_t.columns).sort_values(ascending=False).head(6)\n",
    "feat_names = [f if len(f) <= 28 else f[:25] + \"...\" for f in imp_series.index]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "ypos = np.arange(len(imp_series))\n",
    "plt.barh(ypos, imp_series.values)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.yticks(ypos, feat_names)\n",
    "for i, v in enumerate(imp_series.values):\n",
    "    plt.text(v + 0.001, i, f\"{v:.3f}\", va=\"center\")\n",
    "plt.xlabel(\"Impurity importance\")\n",
    "plt.title(f\"Random Forest top 6 features by impurity importance at {H} days\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7741a56",
   "metadata": {},
   "source": [
    "#### **Key Clinical Variables**\n",
    "\n",
    "* **Neurological Status**: `GCS_last` and `GCS_highest` measure the patient's level of consciousness. Low scores indicate severe impairment.\n",
    "* **Renal Function**: `BUN` (Blood Urea Nitrogen) and `Creatinine` are key markers for kidney stress or failure.\n",
    "* **Respiratory Failure**: `MechVentDuration` indicates how long a patient required mechanical breathing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8927409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct validation transformed features and evaluable mask for the 7-day horizon\n",
    "H = 7\n",
    "m_va = ARTIFACTS[\"labels_val\"][H][\"mask\"]\n",
    "y_va = ARTIFACTS[\"labels_val\"][H][\"y_true\"][m_va].to_numpy()\n",
    "Xva_t = Xt_val.loc[m_va].reindex(columns=Xtr_t.columns, fill_value=0)\n",
    "\n",
    "# Pick top 6 features by impurity importance for plotting\n",
    "top_feats = (\n",
    "    pd.Series(rf.feature_importances_, index=Xtr_t.columns)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(6)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "# Compute ΔAUROC per feature by shuffling that column n_repeats times\n",
    "def permutation_deltas(estimator, X, y, features, n_repeats=30, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    base = roc_auc_score(y, estimator.predict_proba(X)[:, 1])\n",
    "    deltas = []\n",
    "    for f in features:\n",
    "        drops = []\n",
    "        for _ in range(n_repeats):\n",
    "            Xp = X.copy()\n",
    "            Xp[f] = Xp[f].sample(frac=1.0, random_state=int(rng.integers(1 << 31))).to_numpy()\n",
    "            score = roc_auc_score(y, estimator.predict_proba(Xp)[:, 1])\n",
    "            drops.append(base - score)  # positive means AUROC dropped when f was shuffled\n",
    "        deltas.append(np.array(drops))\n",
    "    return deltas\n",
    "\n",
    "box_data = permutation_deltas(rf, Xva_t, y_va, top_feats, n_repeats=30, seed=SEED)\n",
    "labels = [f if len(f) <= 28 else f[:25] + \"...\" for f in top_feats]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.boxplot(box_data, vert=True, labels=labels, manage_ticks=True)\n",
    "plt.axhline(0, linestyle=\"--\", linewidth=1)\n",
    "plt.ylabel(\"Δ AUROC when permuted\")\n",
    "plt.title(f\"Permutation importance distribution at {H} days\")\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91a320",
   "metadata": {},
   "source": [
    "* **Impurity Importance (Bar Chart)**\n",
    "    This plot shows that the model most frequently uses **neurological status (GCS)** and **renal markers (BUN, Creatinine)** to split patients into different risk groups. This means acute brain and kidney dysfunction are the dominant signals for 7-day mortality risk.\n",
    "\n",
    "* **Permutation Importance (Box Plot)**\n",
    "    This plot confirms that **`GCS_last` is the single most reliable predictor**. Randomly shuffling its values causes the biggest drop in model performance. Prolonged ventilation (`MechVentDuration`) and lack of neurological improvement (`GCS_highest`) are also confirmed as stable, important risk factors.\n",
    "\n",
    "#### Clinical Takeaway\n",
    "For predicting short-term ICU mortality, the model confirms that **acute neurological depression and signs of kidney stress are the most critical early warnings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2318e",
   "metadata": {},
   "source": [
    "### **Analysis 2: Risk Stratification on the Test Set**\n",
    "\n",
    "This is a powerful calibration check. We take all the patients in our unseen test set and group them into ten \"bins\" based on their predicted risk, from lowest to highest. Then, we calculate the actual death rate for each bin.\n",
    "\n",
    "* **What this does**: Reports the average predicted risk and the actual (observed) death rate for each patient bin.\n",
    "* **What to look for**: A clear \"staircase\" pattern. The bin with the highest predicted risk should also have the highest observed death rate. This confirms that when the model predicts a high risk, it corresponds to a real-world high risk, making the model's scores trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the evaluable mask and probabilities from rf_pred_test for the 7-day horizon\n",
    "H = 7\n",
    "m_te = ARTIFACTS[\"labels_test\"][H][\"mask\"]\n",
    "yb_te = ARTIFACTS[\"labels_test\"][H][\"y_true\"][m_te].to_numpy()\n",
    "p_te = ARTIFACTS[\"rf_pred_test\"][f\"RF_Risk_{H}d\"][m_te].to_numpy()\n",
    "\n",
    "# Deciles of risk with observed event rates\n",
    "q = pd.qcut(p_te, q=10, labels=False, duplicates=\"drop\")\n",
    "dec = pd.DataFrame({\"decile\": q, \"y\": yb_te, \"p\": p_te})\n",
    "dec_summ = dec.groupby(\"decile\", as_index=False).agg(\n",
    "    n=(\"y\", \"size\"),\n",
    "    risk_mean=(\"p\", \"mean\"),\n",
    "    event_rate=(\"y\", \"mean\")\n",
    ").sort_values(\"decile\", ascending=False)\n",
    "dec_summ[\"risk_mean\"] = dec_summ[\"risk_mean\"].round(3)\n",
    "dec_summ[\"event_rate\"] = dec_summ[\"event_rate\"].round(3)\n",
    "display(dec_summ.style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8327d",
   "metadata": {},
   "source": [
    "### **Analysis 3: Finding a Practical Decision Threshold**\n",
    "\n",
    "A risk score is useful, but a clinical decision is often binary (e.g., \"start intervention\" or \"continue monitoring\"). To use our model in practice, we need to choose a risk **threshold** to turn the probability score into an action.\n",
    "\n",
    "* **What this does**: We'll test a range of thresholds and choose one that achieves a good balance. A common clinical goal is to catch most of the high-risk patients (**high sensitivity/recall**) without flagging too many low-risk patients by mistake (**avoiding false positives**). We will then create a **confusion matrix** to see the concrete numbers of correct and incorrect predictions at that chosen threshold.\n",
    "* **What to look for**: Whether the chosen threshold delivers acceptable sensitivity for clinicians, and whether the number of false positives (false alarms) is manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c92e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two simple thresholding strategies at 7 days\n",
    "# A) Sensitivity target 0.80 if achievable\n",
    "# B) Maximize F1 if sensitivity target is not achievable\n",
    "# Use only evaluable patients for 7-day horizon\n",
    "\n",
    "H = 7\n",
    "col = f\"RF_Risk_{H}d\"\n",
    "\n",
    "# 1) Build a clean, aligned test set frame for the chosen horizon\n",
    "mask = ARTIFACTS[\"labels_test\"][H][\"mask\"]\n",
    "y_series = ARTIFACTS[\"labels_test\"][H][\"y_true\"]\n",
    "\n",
    "# prefer calibrated RF if present\n",
    "rf_pred_test_df = ARTIFACTS.get(\"rf_pred_test_cal\", ARTIFACTS[\"rf_pred_test\"])\n",
    "\n",
    "aligned = pd.DataFrame({\n",
    "    \"y\": y_series,\n",
    "    \"p\": rf_pred_test_df[col]\n",
    "}).loc[mask].dropna()\n",
    "\n",
    "y_te = aligned[\"y\"].to_numpy().astype(int)\n",
    "p_te = aligned[\"p\"].to_numpy().astype(float)\n",
    "\n",
    "# 2) Threshold strategy\n",
    "thresholds = np.linspace(0.02, 0.12, 21)  # focus on plausible region for ~8% prevalence\n",
    "target_sens = 0.80\n",
    "\n",
    "chosen = None\n",
    "for t in thresholds:\n",
    "    yhat = (p_te >= t).astype(int)\n",
    "    sens = recall_score(y_te, yhat, zero_division=0)\n",
    "    if sens >= target_sens:\n",
    "        chosen = (\"sens_0.80\", t)\n",
    "        break\n",
    "\n",
    "if chosen is None:\n",
    "    # fallback to F1\n",
    "    f1s = []\n",
    "    for t in thresholds:\n",
    "        yhat = (p_te >= t).astype(int)\n",
    "        prec = precision_score(y_te, yhat, zero_division=0)\n",
    "        rec = recall_score(y_te, yhat, zero_division=0)\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        f1s.append((t, f1))\n",
    "    t = max(f1s, key=lambda z: z[1])[0]\n",
    "    chosen = (\"f1_max\", t)\n",
    "\n",
    "strategy, tstar = chosen\n",
    "yhat = (p_te >= tstar).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, yhat, labels=[0,1]).ravel()\n",
    "prec = precision_score(y_te, yhat, zero_division=0)\n",
    "rec  = recall_score(y_te, yhat, zero_division=0)\n",
    "spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "brier = brier_score_loss(y_te, p_te)\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"horizon_days\": H,\n",
    "    \"strategy\": strategy,\n",
    "    \"threshold\": round(float(tstar), 3),\n",
    "    \"precision\": round(float(prec), 3),\n",
    "    \"recall_sensitivity\": round(float(rec), 3),\n",
    "    \"specificity\": round(float(spec), 3),\n",
    "    \"brier\": round(float(brier), 3),\n",
    "    \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn), \"tn\": int(tn),\n",
    "    \"n_evaluable\": int(len(y_te)),\n",
    "    \"auroc\": round(float(roc_auc_score(y_te, p_te)), 3)\n",
    "}])\n",
    "\n",
    "from IPython.display import display\n",
    "display(summary.style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930132a3",
   "metadata": {},
   "source": [
    "#### Perfect Sensitivity Creates \"Alert Fatigue\"\n",
    "We already discussed the importance of threshold definition, same principles apply here!\n",
    "\n",
    "* At a risk threshold of 0.05, our model successfully identifies **every single patient** who will die within 7 days (100% sensitivity)\n",
    "* But, **92% of the alerts are false alarms**, and a clinician's phone would be ringing constantly for patients who are not in immediate danger! making the model practically useless as clinicians would quickly learn to ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb5724",
   "metadata": {},
   "source": [
    "### **Analysis 4: Decision Curve Analysis (DCA)**\n",
    "\n",
    "This analysis  measures the \"net benefit\" of using the model to make decisions compared to simpler default strategies.\n",
    "\n",
    "* **What this does**: It plots the net benefit of using the model across a range of risk thresholds against two defaults: \"treat all patients\" and \"treat no patients\".\n",
    "* **What to look for**: The model adds clinical value in regions where its curve is above both the \"treat all\" and \"treat none\" lines. The peak of the curve suggests an optimal threshold range for clinical action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff967a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 4 — Decision Curve with wide-view + validation-driven narrow selection\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, brier_score_loss, roc_auc_score\n",
    "\n",
    "# Reuse utils helpers to keep this lean\n",
    "from utils import align_evaluable, decision_curve_df, select_threshold_by_net_benefit\n",
    "\n",
    "H = 7\n",
    "MODEL = \"RandomForest\"  # \"Cox\" or \"DecisionTree\" also supported\n",
    "\n",
    "# Column names per model\n",
    "col_map = {\n",
    "    \"Cox\":          \"Risk_{h}d\",\n",
    "    \"DecisionTree\": \"DT_Risk_{h}d\",\n",
    "    \"RandomForest\": \"RF_Risk_{h}d\",\n",
    "}\n",
    "# Prefer calibrated predictions if present\n",
    "pred_key_map_test = {\n",
    "    \"Cox\":          \"cph_pred_test_cal\" if \"cph_pred_test_cal\" in ARTIFACTS else \"cph_pred_test\",\n",
    "    \"DecisionTree\": \"dt_pred_test_cal\"  if \"dt_pred_test_cal\"  in ARTIFACTS else \"dt_pred_test\",\n",
    "    \"RandomForest\": \"rf_pred_test_cal\"  if \"rf_pred_test_cal\"  in ARTIFACTS else \"rf_pred_test\",\n",
    "}\n",
    "pred_key_map_val = {\n",
    "    \"Cox\":          \"cph_pred_val_cal\" if \"cph_pred_val_cal\" in ARTIFACTS else \"cph_pred_val\",\n",
    "    \"DecisionTree\": \"dt_pred_val_cal\"  if \"dt_pred_val_cal\"  in ARTIFACTS else \"dt_pred_val\",\n",
    "    \"RandomForest\": \"rf_pred_val_cal\"  if \"rf_pred_val_cal\"  in ARTIFACTS else \"rf_pred_val\",\n",
    "}\n",
    "\n",
    "col = col_map[MODEL].format(h=H)\n",
    "\n",
    "# Align to evaluable cohorts\n",
    "y_va, p_va = align_evaluable(ARTIFACTS[\"labels_val\"][H],  ARTIFACTS[pred_key_map_val[MODEL]],  col)\n",
    "y_te, p_te = align_evaluable(ARTIFACTS[\"labels_test\"][H], ARTIFACTS[pred_key_map_test[MODEL]], col)\n",
    "\n",
    "# 1) Wide view for context on TEST only (descriptive, not used for selection)\n",
    "ths_wide = np.linspace(0.01, 0.50, 26)\n",
    "nb_wide = decision_curve_df(y_te, p_te, ths_wide)\n",
    "\n",
    "# 2) Choose ONE threshold on VALIDATION using a prevalence-informed narrow range\n",
    "prev = float(y_va.mean())\n",
    "low  = max(0.01, round(prev / 4, 3))       # for ~8% prevalence → 0.02\n",
    "high = min(0.50, round(prev + 0.04, 3))    # for ~8% prevalence → 0.12\n",
    "ths_narrow = np.linspace(low, high, 21)\n",
    "\n",
    "t_star = select_threshold_by_net_benefit(y_va, p_va, ths_narrow)\n",
    "\n",
    "# Build narrow curve on TEST for a readable zoom\n",
    "nb_narrow = decision_curve_df(y_te, p_te, ths_narrow)\n",
    "\n",
    "# Plot A — wide curve with plausible range shaded and t* annotated\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(nb_wide[\"threshold\"], nb_wide[\"nb_model\"], marker=\"o\", label=MODEL)\n",
    "plt.plot(nb_wide[\"threshold\"], nb_wide[\"nb_all\"], linestyle=\"--\", label=\"Treat all\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", label=\"Treat none\")\n",
    "plt.axvspan(low, high, color=\"lightgray\", alpha=0.25, label=\"Plausible range\")\n",
    "plt.axvline(t_star, color=\"tab:red\", linestyle=\":\", label=f\"Chosen t={t_star:.3f}\")\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.title(f\"Decision curve at {H} days — wide view\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot B — zoomed curve within the plausible range and t* annotated\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(nb_narrow[\"threshold\"], nb_narrow[\"nb_model\"], marker=\"o\", label=MODEL)\n",
    "plt.plot(nb_narrow[\"threshold\"], nb_narrow[\"nb_all\"], linestyle=\"--\", label=\"Treat all\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", label=\"Treat none\")\n",
    "plt.axvline(t_star, color=\"tab:red\", linestyle=\":\", label=f\"Chosen t={t_star:.3f}\")\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.title(f\"Decision curve at {H} days — narrow view\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Capacity-aware summary at t* on TEST\n",
    "yhat = (p_te >= t_star).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, yhat, labels=[0,1]).ravel()\n",
    "alerts_per_100 = 100 * yhat.mean()\n",
    "summary = pd.DataFrame([{\n",
    "    \"horizon_days\": H,\n",
    "    \"chosen_threshold\": round(t_star, 3),\n",
    "    \"alerts_per_100_patients\": round(float(alerts_per_100), 1),\n",
    "    \"true_positives\": int(tp),\n",
    "    \"false_positives\": int(fp),\n",
    "    \"false_negatives\": int(fn),\n",
    "    \"true_negatives\": int(tn),\n",
    "    \"test_set_auroc\": round(float(roc_auc_score(y_te, p_te)), 3),\n",
    "    \"brier_test\": round(float(brier_score_loss(y_te, p_te)), 3),\n",
    "    \"prevalence_test\": round(float(y_te.mean()), 3)\n",
    "}])\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c78f9",
   "metadata": {},
   "source": [
    "The \"net benefit\" on the y-axis measures how much better a strategy is than doing nothing. We are comparing:\n",
    "* **Our Model (Random Forest)**: The blue line\n",
    "* **Default 1: Treat All Patients**: The steep, downward-sloping dashed line\n",
    "* **Default 2: Treat No Patients**: The flat line at zero\n",
    "\n",
    "The analysis is clear: our model's blue line is almost always **at or below** the \"Treat None\" line. This means that for nearly any decision threshold, we get more benefit from simply sticking to the default strategy of not intervening based on the model's prediction\n",
    "\n",
    "#### **The Clinical Bottom Line**\n",
    "\n",
    "* **No Added Value at 7 Days**: The DCA shows that, despite its accuracy, the model provides little to no clinical utility for making decisions at the 7-day mark. Following its recommendations would not lead to better outcomes than standard care (\"Treat None\") and would be much worse than indiscriminately treating everyone.\n",
    "* **A Tool for Triage, Not Treatment**: This result doesn't mean the model is useless, but it confirms it should not be used as a standalone trigger for a clinical action. Its value lies in identifying patients who may need closer attention, not in dictating their treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146aba62",
   "metadata": {},
   "source": [
    "#### Evalautin more useful thresholds\n",
    "\n",
    "Let's use a more refined, two-step strategy, common in clinical models development:\n",
    "\n",
    "1.  **Find the Best Threshold on Validation Data**: Instead of looking at all thresholds, we will focus on a narrow, clinically realistic range. We will use the **validation set** to find the single threshold that provides the highest \"net benefit\"\n",
    "2.  **Verify on Test Data**: We will take this single, pre-chosen threshold and apply it to the **unseen test set**. This gives us an honest estimate of how the model would perform in practice, including its real-world benefit and the workload it would create (i.e., \"alerts per 100 patients\")\n",
    "\n",
    "This method prevents us from \"cherry-picking\" a threshold that only looks good on the test data and provides a more realistic assessment of the model's value\n",
    "\n",
    "### How do we choose our threshold range, in this specific case?\n",
    "\n",
    "As we know, the selection of this range is practitioner driven. However, we could make some sensical assumptions:\n",
    "\n",
    "- The actual death rate (prevalence) in our 7-day test set is around 8%. A useful decision threshold must be near this value. It makes no sense to have an alert trigger at a 50% risk if almost no patients have a risk that high, and the actual event rate is much lower. We could estimate a range between 2% to 12% focuseing on this zone of practical decision-making\n",
    "- In an ICU, missing a dying patient (a false negative) is clearly far worse than a false alarm (a false positive). Our low threshold range aims at reflecting this\n",
    "- It's easy to find a threshold that looks perfect on the test data by chance. To ensure our results are honest and reproducible, we follow this rule:\n",
    "\n",
    "1.  **Select on Validation**: We test all our potential thresholds on the **validation set** and find the single one (`t_star`) that provides the highest net benefit\n",
    "2.  **Report on Test**: We then take this *one*, pre-selected threshold and apply it a single time to the **unseen test set**. The results from this final step are our best, honest, unbiased estimate of how the model would perform in the real world\n",
    "\n",
    "This two-step process prevents \"cheating\" by tuning our decision rule to the specific quirks of the test data, giving us a much more realistic assessment of the model's true value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c36373",
   "metadata": {},
   "source": [
    "* **A more clear Clinical benefit**\n",
    "    The plot now shows that in the most relevant threshold range (between 2% and 12% risk), the Random Forest model (blue line) is superior to both default strategies of \"Treat All\" and \"Treat None\"\n",
    "\n",
    "* **An Optimal Operating Point**\n",
    "    The red dotted line shows the optimal threshold of **~0.074**, which was selected on the validation set because it offered the highest net benefit\n",
    "\n",
    "* **A Two-Step Triage Policy**\n",
    "    The most effective way to use this model is as a **supportive screening tool**, not a standalone tool. A practical policy could be:\n",
    "    1.  Patients with a risk score above the chosen threshold (~0.074) get a priority clinician review\n",
    "    2.  An intervention is only escalated if the clinician confirms the high risk with other bedside criteria (e.g., low GCS, rising BUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f02874",
   "metadata": {},
   "source": [
    "### **Analysis 4A: Decision Curve Analysis (Wide View)**\n",
    "\n",
    "This plot helps us understand the model's value across a **wide range of possibilities**. It shows where our model might be useful and where it is not.\n",
    "\n",
    "* **What this does**:\n",
    "    * It plots the model's net benefit across a broad range of risk thresholds (from 1% to 50%).\n",
    "    * It highlights a **\"plausible range\"** (shaded gray area) where a clinical decision is most likely to be made, based on the actual event rate in our validation data.\n",
    "    * It marks the single **best threshold** (red dotted line) that was identified using only the validation set.\n",
    "* **What to look for**:\n",
    "    * We want to see the model's blue line above the other two lines, especially inside the shaded plausible range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d34cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide-view DCA with validation-chosen threshold and plausible band\n",
    "\n",
    "H = 7\n",
    "MODEL = \"RandomForest\"  # \"Cox\" or \"DecisionTree\" also supported\n",
    "\n",
    "# Column names per model\n",
    "col_map = {\"Cox\": \"Risk_{h}d\", \"DecisionTree\": \"DT_Risk_{h}d\", \"RandomForest\": \"RF_Risk_{h}d\"}\n",
    "col = col_map[MODEL].format(h=H)\n",
    "\n",
    "# Prefer calibrated predictions when available\n",
    "pred_key_map_test = {\n",
    "    \"Cox\": \"cph_pred_test_cal\" if \"cph_pred_test_cal\" in ARTIFACTS else \"cph_pred_test\",\n",
    "    \"DecisionTree\": \"dt_pred_test_cal\" if \"dt_pred_test_cal\" in ARTIFACTS else \"dt_pred_test\",\n",
    "    \"RandomForest\": \"rf_pred_test_cal\" if \"rf_pred_test_cal\" in ARTIFACTS else \"rf_pred_test\",\n",
    "}\n",
    "pred_key_map_val = {\n",
    "    \"Cox\": \"cph_pred_val_cal\" if \"cph_pred_val_cal\" in ARTIFACTS else \"cph_pred_val\",\n",
    "    \"DecisionTree\": \"dt_pred_val_cal\" if \"dt_pred_val_cal\" in ARTIFACTS else \"dt_pred_val\",\n",
    "    \"RandomForest\": \"rf_pred_val_cal\" if \"rf_pred_val_cal\" in ARTIFACTS else \"rf_pred_val\",\n",
    "}\n",
    "\n",
    "# Align evaluable cohorts\n",
    "y_va, p_va = align_evaluable(ARTIFACTS[\"labels_val\"][H],  ARTIFACTS[pred_key_map_val[MODEL]],  col)\n",
    "y_te, p_te = align_evaluable(ARTIFACTS[\"labels_test\"][H], ARTIFACTS[pred_key_map_test[MODEL]], col)\n",
    "\n",
    "# Wide thresholds for descriptive context on TEST\n",
    "ths_wide = np.linspace(0.01, 0.50, 26)\n",
    "nb_wide = decision_curve_df(y_te, p_te, ths_wide)\n",
    "\n",
    "# Prevalence-informed plausible band computed from VALIDATION\n",
    "prev = float(y_va.mean())\n",
    "low  = max(0.01, round(prev / 4, 3))     # ~2% for ~8% prevalence\n",
    "high = min(0.50, round(prev + 0.04, 3))  # ~12% for ~8% prevalence\n",
    "\n",
    "# Choose ONE threshold on VALIDATION within the plausible band\n",
    "ths_narrow = np.linspace(low, high, 21)\n",
    "t_star = select_threshold_by_net_benefit(y_va, p_va, ths_narrow)\n",
    "\n",
    "# Plot wide view with band and chosen threshold\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(nb_wide[\"threshold\"], nb_wide[\"nb_model\"], marker=\"o\", label=MODEL)\n",
    "plt.plot(nb_wide[\"threshold\"], nb_wide[\"nb_all\"], linestyle=\"--\", label=\"Treat all\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", label=\"Treat none\")\n",
    "plt.axvspan(low, high, color=\"lightgray\", alpha=0.25, label=\"Plausible range\")\n",
    "plt.axvline(t_star, color=\"tab:red\", linestyle=\":\", label=f\"Chosen t={t_star:.3f}\")\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.title(f\"Decision curve at {H} days — wide view\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clinical Insights (Wide View)**\n",
    "\n",
    "The plot shows the model adds value primarily as a **screening tool**. The blue line is above the other strategies only at very low risk thresholds, as highlighted by the shaded \"plausible range\". Outside this band, the model offers no benefit over standard care (\"Treat none\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0dfb63",
   "metadata": {},
   "source": [
    "### **Analysis 4B: Decision Curve (Narrow View) & Capacity Summary**\n",
    "\n",
    "Now we zoom in on the **clinically plausible band** to confirm the model's value and then translate our chosen threshold into practical, operational numbers.\n",
    "\n",
    "* **What this does**:\n",
    "    * Re-plots the decision curve, focusing only on the 2% to 12% risk threshold range.\n",
    "    * Creates a summary table that shows the real-world impact of using our chosen threshold (`t_star`) on the test data, including **alerts per 100 patients**.\n",
    "* **What to look for**:\n",
    "    * The blue curve should be consistently above the other lines inside this band, confirming its value.\n",
    "    * The summary table tells us the **clinical workload** (how many alerts to expect) and the **trade-offs** (how many patients are correctly identified vs. missed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96327b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow-view DCA and capacity-aware summary on TEST using the same t_star\n",
    "\n",
    "# Build narrow curve on TEST\n",
    "nb_narrow = decision_curve_df(y_te, p_te, ths_narrow)\n",
    "\n",
    "# Plot narrow view\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(nb_narrow[\"threshold\"], nb_narrow[\"nb_model\"], marker=\"o\", label=MODEL)\n",
    "plt.plot(nb_narrow[\"threshold\"], nb_narrow[\"nb_all\"], linestyle=\"--\", label=\"Treat all\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", label=\"Treat none\")\n",
    "plt.axvline(t_star, color=\"tab:red\", linestyle=\":\", label=f\"Chosen t={t_star:.3f}\")\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.title(f\"Decision curve at {H} days — narrow view\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Capacity-aware summary at the chosen threshold on TEST\n",
    "yhat = (p_te >= t_star).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, yhat, labels=[0, 1]).ravel()\n",
    "alerts_per_100 = 100 * yhat.mean()\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"horizon_days\": H,\n",
    "    \"chosen_threshold\": round(float(t_star), 3),\n",
    "    \"alerts_per_100_patients\": round(float(alerts_per_100), 1),\n",
    "    \"true_positives\": int(tp),\n",
    "    \"false_positives\": int(fp),\n",
    "    \"false_negatives\": int(fn),\n",
    "    \"true_negatives\": int(tn),\n",
    "    \"test_set_auroc\": round(float(roc_auc_score(y_te, p_te)), 3),\n",
    "    \"brier_test\": round(float(brier_score_loss(y_te, p_te)), 3),\n",
    "    \"prevalence_test\": round(float(y_te.mean()), 3)\n",
    "}])\n",
    "from IPython.display import display\n",
    "display(summary.style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f854c7",
   "metadata": {},
   "source": [
    "#### **Clinical Insights (Narrow View & Summary)**\n",
    "\n",
    "Within the clinically relevant 2% to 12% band, the model shows **consistent positive net benefit**, supporting its use as a **triage aid**. The summary table quantifies the expected workload at our chosen threshold, which informs staffing and escalation rules. A sensible policy would be **\"screen then confirm\"**: an alert from the model triggers a clinician review, but an intervention only proceeds with corroborating bedside findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a353b8d",
   "metadata": {},
   "source": [
    "### **Analysis 5: Subgroup Fairness Check**\n",
    "\n",
    "This final, crucial check ensures the model performs fairly across different patient populations.\n",
    "\n",
    "* **What this does**: Calculates the model's performance (AUROC) separately for key demographic and clinical subgroups (e.g., Gender, ICU Type).\n",
    "* **What to look for**: The AUROC should be reasonably stable across all subgroups. A large performance drop for a specific group is a red flag for bias and warrants further investigation before the model could ever be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dfffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroup checks for Sex and ICU type on evaluable test cohort\n",
    "# Adjust column names if your dataset uses different ones\n",
    "\n",
    "def subgroup_metric(colname):\n",
    "    out = []\n",
    "    for level, idx in X_test.loc[m_te].groupby(colname).groups.items():\n",
    "        y_sub = y_te[X_test.loc[m_te].index.isin(idx)]\n",
    "        p_sub = p_te[X_test.loc[m_te].index.isin(idx)]\n",
    "        if len(np.unique(y_sub)) < 2:\n",
    "            au = np.nan\n",
    "        else:\n",
    "            au = roc_auc_score(y_sub, p_sub)\n",
    "        out.append({\"group\": colname, \"level\": level, \"n\": int(len(y_sub)), \"auroc\": np.round(au, 3)})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "subs = []\n",
    "for col in [\"Gender\", \"ICUtype\", \"CCU\", \"CSRU\", \"SICU\"]:\n",
    "    if col in X_test.columns:\n",
    "        subs.append(subgroup_metric(col))\n",
    "\n",
    "if len(subs):\n",
    "    sub_tbl = pd.concat(subs, ignore_index=True)\n",
    "    display(sub_tbl.style)\n",
    "else:\n",
    "    print(\"No subgroup columns found among the requested candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f240378",
   "metadata": {},
   "source": [
    "* **Gender Discrepancy**: There is a relative important performance gap between genders. The model is clearly better at ranking risk for the male group (AUROC 0.871) than the other (AUROC 0.778)\n",
    "* However, the model performs very consistently across different surgical ICU types (CSRU and SICU)\n",
    "* Watchout! The model appears to perform best in the CCU group, but with such a small sample, this high score may be due to random chance and should be interpreted cautiously\n",
    "\n",
    "A model that is less accurate for a specific group is an **unfair model**. Relying on it could lead to systematically poorer care for that patient population. While the model is still effective for all groups, the lower performance in one gender group flags a serious limitation that must be addressed before the model is used in a real clinic scenario\n",
    "\n",
    "The same analysis and recommendations from our previous exercises apply here!! **Fairness is as important as several other metrics**!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530a01b",
   "metadata": {},
   "source": [
    "# 7. The Final showdown: Side-by-side comparison across Cox, Decision Tree, and Random Forest\n",
    "\n",
    "The final step is to bring all the results together for a direct, side-by-side comparison\n",
    "\n",
    "* **What we will do**:\n",
    "    1.  **Master Metrics Table**: Combine the test-set performance (AUROC, AUPRC, Brier score) for all three models across all three horizons into a single table\n",
    "    2.  **Performance Plot**: Visualize the AUROC/ AUPRC of each model at each horizon to quickly see which model has the best ranking ability\n",
    "    3.  **Calibration Plot**: Overlay the calibration curves for all three models at the 7-day horizon to see which model's risk scores are the most reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36685ac",
   "metadata": {},
   "source": [
    "## 7.1. Model calibration\n",
    "\n",
    "An accurate model (high AUROC) isn't enough. We also need its risk scores to be **reliable**\n",
    "\n",
    "- We will fit a \"calibrator\" for each model at each horizon using only the **validation set**. This calibrator learns to adjust the raw, often unreliable, probabilities from the models\n",
    "- We then apply this frozen calibrator to the **test set** to get our final, trustworthy risk scores\n",
    "- Tree-based models like Decision Trees and Random Forests are notoriously poorly calibrated out-of-the-box **(Rememeber their raw scores are not probabilities!!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZONS = ARTIFACTS[\"horizons_days\"]\n",
    "\n",
    "# Create copies of the original predictions to store the calibrated versions\n",
    "cph_pred_val_cal  = ARTIFACTS[\"cph_pred_val\"].copy()\n",
    "cph_pred_test_cal = ARTIFACTS[\"cph_pred_test\"].copy()\n",
    "dt_pred_val_cal   = ARTIFACTS[\"dt_pred_val\"].copy()\n",
    "dt_pred_test_cal  = ARTIFACTS[\"dt_pred_test\"].copy()\n",
    "rf_pred_val_cal   = ARTIFACTS[\"rf_pred_val\"].copy()\n",
    "rf_pred_test_cal  = ARTIFACTS[\"rf_pred_test\"].copy()\n",
    "\n",
    "# A dictionary to store the fitted calibrator for each model and horizon\n",
    "calibrators = {\"Cox\": {}, \"DecisionTree\": {}, \"RandomForest\": {}}\n",
    "\n",
    "print(\"Fitting calibrators on validation set for each model and horizon...\")\n",
    "for h in HORIZONS:\n",
    "    # Define the prediction column names for this horizon\n",
    "    col_cph = f\"Risk_{h}d\"\n",
    "    col_dt  = f\"DT_Risk_{h}d\"\n",
    "    col_rf  = f\"RF_Risk_{h}d\"\n",
    "\n",
    "    # --- Step 1: Get the validation data for this horizon ---\n",
    "    # This data will be used to TEACH the calibrator how to adjust the probabilities.\n",
    "    m_va = ARTIFACTS[\"labels_val\"][h][\"mask\"]\n",
    "    y_va = ARTIFACTS[\"labels_val\"][h][\"y_true\"][m_va].to_numpy()\n",
    "\n",
    "    # --- Step 2: Fit a separate calibrator for each model on the validation data ONLY ---\n",
    "    cal_cph = fit_isotonic_calibrator(y_va, ARTIFACTS[\"cph_pred_val\"][col_cph][m_va].to_numpy())\n",
    "    cal_dt  = fit_isotonic_calibrator(y_va, ARTIFACTS[\"dt_pred_val\"][col_dt][m_va].to_numpy())\n",
    "    cal_rf  = fit_isotonic_calibrator(y_va, ARTIFACTS[\"rf_pred_val\"][col_rf][m_va].to_numpy())\n",
    "\n",
    "    # Store the fitted calibrators\n",
    "    calibrators[\"Cox\"][h]          = cal_cph\n",
    "    calibrators[\"DecisionTree\"][h] = cal_dt\n",
    "    calibrators[\"RandomForest\"][h] = cal_rf\n",
    "\n",
    "    # --- Step 3: Apply the FROZEN calibrators to both validation and test sets ---\n",
    "    # Apply to validation set (to see how well it worked)\n",
    "    cph_pred_val_cal.loc[m_va, col_cph] = apply_calibrator(cal_cph, ARTIFACTS[\"cph_pred_val\"][col_cph][m_va].to_numpy())\n",
    "    dt_pred_val_cal.loc[m_va,  col_dt]  = apply_calibrator(cal_dt,  ARTIFACTS[\"dt_pred_val\"][col_dt][m_va].to_numpy())\n",
    "    rf_pred_val_cal.loc[m_va,  col_rf]  = apply_calibrator(cal_rf,  ARTIFACTS[\"rf_pred_val\"][col_rf][m_va].to_numpy())\n",
    "\n",
    "    # Apply to the unseen test set (the final, honest evaluation)\n",
    "    m_te = ARTIFACTS[\"labels_test\"][h][\"mask\"]\n",
    "    cph_pred_test_cal.loc[m_te, col_cph] = apply_calibrator(cal_cph, ARTIFACTS[\"cph_pred_test\"][col_cph][m_te].to_numpy())\n",
    "    dt_pred_test_cal.loc[m_te,  col_dt]  = apply_calibrator(cal_dt,  ARTIFACTS[\"dt_pred_test\"][col_dt][m_te].to_numpy())\n",
    "    rf_pred_test_cal.loc[m_te,  col_rf]  = apply_calibrator(cal_rf,  ARTIFACTS[\"rf_pred_test\"][col_rf][m_te].to_numpy())\n",
    "\n",
    "# --- Step 4: Store the calibrated predictions back into ARTIFACTS ---\n",
    "# We will use these new, more reliable probabilities for our final comparison plots.\n",
    "ARTIFACTS[\"cph_pred_val_cal\"]  = cph_pred_val_cal\n",
    "ARTIFACTS[\"cph_pred_test_cal\"] = cph_pred_test_cal\n",
    "ARTIFACTS[\"dt_pred_val_cal\"]   = dt_pred_val_cal\n",
    "ARTIFACTS[\"dt_pred_test_cal\"]  = dt_pred_test_cal\n",
    "ARTIFACTS[\"rf_pred_val_cal\"]   = rf_pred_val_cal\n",
    "ARTIFACTS[\"rf_pred_test_cal\"]  = rf_pred_test_cal\n",
    "ARTIFACTS[\"calibrators\"]       = calibrators\n",
    "print(\"Calibrators trained on validation set and calibrated predictions stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d38496",
   "metadata": {},
   "source": [
    "## 7.2. The Final showdown: Comparing our calibrated models\n",
    "\n",
    "* **What we'll do**:\n",
    "    1.  **Master Metrics Table**: Combine the final test-set performance (AUROC, AUPRC, Brier score) for all three models into a single summary table\n",
    "    2.  **Performance Plots**: Visualize the AUROC (ranking ability) and AUPRC (performance on rare events) of each model to quickly see which performs best\n",
    "    3.  **Final Calibration Plot**: Overlay the calibration curves for all three models at the 7-day horizon to confirm that our calibration step was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2885eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZONS = ARTIFACTS[\"horizons_days\"]\n",
    "\n",
    "rows = []\n",
    "# --- Step 1: Build a master table with calibrated metrics ---\n",
    "for h in HORIZONS:\n",
    "    y_te = ARTIFACTS[\"labels_test\"][h][\"y_true\"].to_numpy()\n",
    "    m_te = ARTIFACTS[\"labels_test\"][h][\"mask\"]\n",
    "\n",
    "    # Calculate metrics for each model using the calibrated predictions\n",
    "    rows.append({\"model\": \"Cox\", \"horizon_days\": h, **fixed_horizon_metrics(\n",
    "        y_te, ARTIFACTS[\"cph_pred_test_cal\"][f\"Risk_{h}d\"], m_te)})\n",
    "\n",
    "    rows.append({\"model\": \"DecisionTree\", \"horizon_days\": h, **fixed_horizon_metrics(\n",
    "        y_te, ARTIFACTS[\"dt_pred_test_cal\"][f\"DT_Risk_{h}d\"], m_te)})\n",
    "\n",
    "    rows.append({\"model\": \"RandomForest\", \"horizon_days\": h, **fixed_horizon_metrics(\n",
    "        y_te, ARTIFACTS[\"rf_pred_test_cal\"][f\"RF_Risk_{h}d\"], m_te)})\n",
    "\n",
    "cmp_tbl = pd.DataFrame(rows).sort_values([\"horizon_days\", \"model\"]).reset_index(drop=True)\n",
    "for c in [\"auroc\", \"auprc\", \"brier\"]:\n",
    "    cmp_tbl[c] = cmp_tbl[c].round(3)\n",
    "display(cmp_tbl.style)\n",
    "\n",
    "\n",
    "# --- Step 2: Plot grouped bars for AUROC and AUPRC ---\n",
    "model_order = [\"Cox\", \"DecisionTree\", \"RandomForest\"]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "def plot_grouped_bars(metric_name):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    xpos = np.arange(len(HORIZONS))\n",
    "    width = 0.25\n",
    "    for j, m in enumerate(model_order):\n",
    "        vals = [float(cmp_tbl.query(\"horizon_days == @h and model == @m\")[metric_name]) for h in HORIZONS]\n",
    "        plt.bar(xpos + (j - 1)*width, vals, width=width, label=m, color=colors[j])\n",
    "    plt.xticks(xpos, [f\"{h} Days\" for h in HORIZONS])\n",
    "    plt.ylabel(metric_name.upper())\n",
    "    plt.title(f\"{metric_name.upper()} by Model and Horizon (Calibrated Test Set)\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grouped_bars(\"auroc\")\n",
    "plot_grouped_bars(\"auprc\")\n",
    "\n",
    "\n",
    "# --- Step 3: Plot final calibration overlay at 7 days ---\n",
    "h = 7\n",
    "mask = ARTIFACTS[\"labels_test\"][h][\"mask\"]\n",
    "y = ARTIFACTS[\"labels_test\"][h][\"y_true\"][mask].to_numpy()\n",
    "\n",
    "probs_calibrated = {\n",
    "    \"Cox\":          ARTIFACTS[\"cph_pred_test_cal\"][f\"Risk_{h}d\"][mask].to_numpy(),\n",
    "    \"DecisionTree\": ARTIFACTS[\"dt_pred_test_cal\"][f\"DT_Risk_{h}d\"][mask].to_numpy(),\n",
    "    \"RandomForest\": ARTIFACTS[\"rf_pred_test_cal\"][f\"RF_Risk_{h}d\"][mask].to_numpy(),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Perfect Calibration\")\n",
    "for i, (name, p) in enumerate(probs_calibrated.items()):\n",
    "    if y.size > 0 and len(np.unique(y)) > 1:\n",
    "        frac_pos, prob_mean = calibration_curve(y, p, n_bins=10, strategy=\"quantile\")\n",
    "        plt.plot(prob_mean, frac_pos, marker=\"o\", label=name, color=colors[i])\n",
    "\n",
    "plt.xlabel(\"Predicted Risk (Mean of Bin)\")\n",
    "plt.ylabel(\"Observed Event Rate\")\n",
    "plt.title(\"Final Calibration at 7 Days (Test Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae831666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu-survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
