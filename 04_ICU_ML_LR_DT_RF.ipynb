{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6df2cb",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "# ML for Healthcare with Weights & Biases: Interpretable Baselines and Clinical Evaluation\n",
    "\n",
    "We continue our focus on shifting from coding models to *understanding* them — how well they perform, how trustworthy their probabilities are, and how they behave across patient groups. Strenghtening the foundation for clinically aware ML practice.\n",
    "\n",
    "**Objective**  \n",
    "Build, track, and interpret models predicting in-hospital mortality in ICU patients using Weights & Biases (W&B).  \n",
    "This notebook transforms standard Machine Learning (ML) practice into an auditable, interpretable, and clinically meaningful workflow\n",
    "\n",
    "**You will learn**\n",
    "- How to track model training and evaluation runs using Weights & Biases  \n",
    "- How to interpret models and understand their calibration  \n",
    "- How to examine fairness and subgroup performance  \n",
    "- How to communicate model insights for healthcare decisions\n",
    "\n",
    "**Models**\n",
    "We’ll compare three interpretable baselines:\n",
    "1. **Logistic Regression**: simple linear reference, easy to explain  \n",
    "2. **Decision Tree (shallow)**: intuitive splits, visually transparent  \n",
    "3. **Random Forest**: robust ensemble, main focus for tuning and interpretability\n",
    "\n",
    "**Dataset**\n",
    "[PhysioNet Challenge 2012 dataset](https://physionet.org/content/challenge-2012/1.0.0/), containing clinical measurements, demographics, and the target:  \n",
    "`In-hospital_death` (binary: 1 = patient died during stay, 0 = survived).\n",
    "\n",
    "**Weights & Biases**\n",
    "Used to:\n",
    "- Track configurations, metrics, and plots  \n",
    "- Compare models and hyperparameters  \n",
    "- Log interpretability results (feature importance, calibration, subgroups)  \n",
    "- Support model transparency and documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201233",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca632bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports, reproducibility, and Weights & Biases initialization\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "# Initialize Weights & Biases\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    average_precision_score, \n",
    "    brier_score_loss,\n",
    "    roc_curve, \n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login\n",
    "# !wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_PROJECT = \"ml-healthcare-intro\"\n",
    "\n",
    "# wandb.login() # Uncomment if not logged in\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT, \n",
    "    job_type=\"setup\", \n",
    "        config={\n",
    "        \"seed\": SEED,\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"dataset\": \"physionet2012_set_a\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log environment metadata\n",
    "wandb.config.update({\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"pandas_version\": pd.__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "}, allow_val_change=True)\n",
    "\n",
    "print(f\"Weights & Biases tracking URL: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ffd0e",
   "metadata": {},
   "source": [
    "# 3. Data loading and initial checks\n",
    "We will load the dataset, confirm the target, and log basic summaries to Weights & Biases. This lets us explore class balance, missingness, and a quick data preview directly in the dashboard.\n",
    "\n",
    "- Use the Weights & Biases tables to examine class balance, missingness, and a data preview  \n",
    "- Check that the target definition and event rate look reasonable before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "PATH = \"PhysionetChallenge2012-set-a.csv.gz\"\n",
    "\n",
    "# Simple check to ensure the data file exists before trying to load it\n",
    "if not os.path.exists(PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Error: The data file was not found at '{PATH}'. \"\n",
    "        \"Please ensure the dataset is in the correct directory.\"\n",
    "    )\n",
    "\n",
    "ICU = pd.read_csv(PATH, compression=\"gzip\")\n",
    "\n",
    "TARGET = \"In-hospital_death\"\n",
    "ID_COL = \"recordid\" if \"recordid\" in ICU.columns else None\n",
    "\n",
    "if TARGET not in ICU.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in dataset\")\n",
    "\n",
    "# Ensure target is numeric and binary\n",
    "ICU[TARGET] = pd.to_numeric(ICU[TARGET], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Basic facts\n",
    "n_rows, n_cols = ICU.shape\n",
    "pos_rate = float(ICU[TARGET].mean())\n",
    "\n",
    "# Class balance table\n",
    "cb_series = ICU[TARGET].value_counts().sort_index()\n",
    "class_balance_tbl = wandb.Table(data=[[int(k), int(v), float(v / n_rows)] for k, v in cb_series.items()],\n",
    "                                columns=[\"label\", \"count\", \"fraction\"])\n",
    "\n",
    "# Missingness table (top 30)\n",
    "miss = ICU.isna().mean().sort_values(ascending=False)\n",
    "miss_top = miss.head(30).reset_index()\n",
    "miss_top.columns = [\"column\", \"missing_fraction\"]\n",
    "missing_tbl = wandb.Table(data=miss_top.values.tolist(), columns=list(miss_top.columns))\n",
    "\n",
    "# Data preview table (sample up to 200 rows for UI responsiveness)\n",
    "preview = ICU.sample(n=min(200, len(ICU)), random_state=SEED)\n",
    "preview_tbl = wandb.Table(dataframe=preview)\n",
    "\n",
    "wandb.log({\n",
    "    \"dataset_rows\": n_rows,\n",
    "    \"dataset_cols\": n_cols,\n",
    "    \"positive_rate\": pos_rate,\n",
    "    \"class_balance_table\": class_balance_tbl,\n",
    "    \"missingness_top30_table\": missing_tbl,\n",
    "    \"data_preview_table\": preview_tbl\n",
    "})\n",
    "\n",
    "print(f\"Loaded ICU with shape {ICU.shape} and positive rate {pos_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68fdbf",
   "metadata": {},
   "source": [
    "# 4. Simple preprocessing\n",
    "### Preprocessing and splits\n",
    "We will split the data into train, validation, and test sets, impute missing values and one-hot encode categorical variables, and log feature lists and split sizes to Weights & Biases for transparency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: split data and prepare simple pipelines\n",
    "\n",
    "# Drop ID column if present\n",
    "X = ICU.drop(columns=[c for c in [TARGET, ID_COL] if c in ICU.columns])\n",
    "y = ICU[TARGET]\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# Split data (60 percent train, 20 percent validation, 20 percent test)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, stratify=y_train_full, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Define transformations\n",
    "num_transformer = SimpleImputer(strategy=\"median\")\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, num_cols),\n",
    "        (\"cat\", cat_transformer, cat_cols)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "# Log split sizes\n",
    "wandb.log({\n",
    "    \"train_rows\": len(X_train),\n",
    "    \"val_rows\": len(X_val),\n",
    "    \"test_rows\": len(X_test),\n",
    "    \"n_num_features_raw\": len(num_cols),\n",
    "    \"n_cat_features_raw\": len(cat_cols),\n",
    "    \"n_features_transformed\": X_train_t.shape[1]\n",
    "})\n",
    "\n",
    "# Log feature lists as W&B Tables for inspectability\n",
    "num_tbl = wandb.Table(data=[[c, \"numeric\"] for c in num_cols], columns=[\"feature\", \"type\"])\n",
    "cat_tbl = wandb.Table(data=[[c, \"categorical\"] for c in cat_cols], columns=[\"feature\", \"type\"])\n",
    "wandb.log({\"feature_list_numeric\": num_tbl, \"feature_list_categorical\": cat_tbl})\n",
    "\n",
    "print(\"Preprocessing complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01940624",
   "metadata": {},
   "source": [
    "Use the feature tables and split sizes in Weights & Biases to verify preprocessing choices  \n",
    "All models next will consume the same transformed matrices for fair comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6087c95",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression: establishing a simple reference\n",
    "\n",
    "Before exploring complex models, it’s helpful to start with a simple and interpretable baseline. Logistic Regression gives a linear relationship between features and the log-odds of the outcome. Helping us understand whether more flexible models (like Random Forests) truly add value.\n",
    "\n",
    "1. We’ll train a Logistic Regression model, evaluate it on the validation and test sets\n",
    "2. Log all metrics to Weights & Biases to compare later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression baseline with ROC/PR logged as W&B Tables\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    config={\"model\": \"logistic_regression\", \"seed\": SEED},\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "# Train\n",
    "log_reg = LogisticRegression(max_iter=500, solver=\"liblinear\", random_state=SEED)\n",
    "log_reg.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = log_reg.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = log_reg.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# ROC and PR curve points as Tables for interactive plots in W&B\n",
    "fpr, tpr, roc_thresh = roc_curve(y_val, y_val_prob)\n",
    "roc_table = wandb.Table(data=list(zip(fpr, tpr, roc_thresh)), columns=[\"fpr\", \"tpr\", \"threshold\"])\n",
    "wandb.log({\"roc_curve_val\": roc_table})\n",
    "\n",
    "prec, rec, pr_thresh = precision_recall_curve(y_val, y_val_prob)\n",
    "# sklearn returns thresholds length one less than precision/recall, pad with None for table alignment\n",
    "pr_table = wandb.Table(data=list(zip(rec, prec, list(pr_thresh) + [None])), columns=[\"recall\", \"precision\", \"threshold\"])\n",
    "wandb.log({\"pr_curve_val\": pr_table})\n",
    "\n",
    "# Coefficients for transparency\n",
    "coef_df = pd.DataFrame({\"feature\": X_train_t.columns, \"coefficient\": log_reg.coef_[0]})\n",
    "coef_tbl = wandb.Table(dataframe=coef_df.sort_values(\"coefficient\", ascending=False))\n",
    "wandb.log({\"log_reg_coefficients\": coef_tbl})\n",
    "\n",
    "# Predictions table sample for later slicing in the UI\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index if ID_COL is None else X_val.index,  # keep index for traceability\n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"LR validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b057b",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "Open the Weights & Biases project  \n",
    "Use the ROC and PR tables to create interactive line plots for this run  \n",
    "Inspect the coefficients table for a first look at feature effects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859adf44",
   "metadata": {},
   "source": [
    "# 6. Decision Tree baseline\n",
    "\n",
    "A shallow tree is easy to read and helps us see simple non-linear rules. We will train a small tree, log metrics, curve points, feature importances, and a predictions sample to Weights & Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Decision Tree baseline\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    config={\n",
    "        \"model_type\": \"decision_tree\",\n",
    "        \"seed\": SEED,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_samples_leaf\": 20\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "# Train a small, readable tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=SEED\n",
    ")\n",
    "dt.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = dt.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = dt.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)                    # Area under ROC\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)          # Area under PR\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)               # Calibration error\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# ROC and PR curve points as Tables\n",
    "fpr, tpr, roc_thresh = roc_curve(y_val, y_val_prob)\n",
    "wandb.log({\"roc_curve_val\": wandb.Table(data=list(zip(fpr, tpr, roc_thresh)), columns=[\"fpr\", \"tpr\", \"threshold\"])})\n",
    "\n",
    "prec, rec, pr_thresh = precision_recall_curve(y_val, y_val_prob)\n",
    "wandb.log({\"pr_curve_val\": wandb.Table(data=list(zip(rec, prec, list(pr_thresh) + [None])), columns=[\"recall\", \"precision\", \"threshold\"])})\n",
    "\n",
    "# Feature importances\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": X_train_t.columns, \"importance\": dt.feature_importances_})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# Predictions sample for slicing in the UI\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index,\n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"DT validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb201b",
   "metadata": {},
   "source": [
    "# 7. Random Forest baseline\n",
    "### Random Forest baseline\n",
    "\n",
    "As we know, a Random Forest averages many trees to improve stability and performance. We will train a baseline model and log metrics, curve points, feature importances, and a predictions sample to W&B. Later we will tune hyperparameters with a short sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Random Forest baseline\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    config={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"seed\": SEED,\n",
    "        \"n_estimators\": 300,\n",
    "        \"max_depth\": None,\n",
    "        \"max_features\": \"sqrt\",\n",
    "        \"min_samples_leaf\": 5,\n",
    "        \"n_jobs\": -1\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    max_features=\"sqrt\",\n",
    "    min_samples_leaf=5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = rf.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = rf.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)                    # Area under ROC\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)          # Area under PR\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)               # Calibration error\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# ROC and PR curve points as Tables\n",
    "fpr, tpr, roc_thresh = roc_curve(y_val, y_val_prob)\n",
    "wandb.log({\"roc_curve_val\": wandb.Table(data=list(zip(fpr, tpr, roc_thresh)), columns=[\"fpr\", \"tpr\", \"threshold\"])})\n",
    "\n",
    "prec, rec, pr_thresh = precision_recall_curve(y_val, y_val_prob)\n",
    "wandb.log({\"pr_curve_val\": wandb.Table(data=list(zip(rec, prec, list(pr_thresh) + [None])), columns=[\"recall\", \"precision\", \"threshold\"])})\n",
    "\n",
    "# Feature importances\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": X_train_t.columns, \"importance\": rf.feature_importances_})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# Predictions sample for slicing in the UI\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index,\n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"RF validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f4af3",
   "metadata": {},
   "source": [
    "#### TO DO\n",
    "- Use the Weights & Biases compare view to contrast Logistic Regression, Decision Tree, and Random Forest  \n",
    "- Check whether the Random Forest improves area under ROC, area under PR, and calibration score  \n",
    "- Review feature importances to see if top drivers align with clinical sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95542b",
   "metadata": {},
   "source": [
    "### 7.1. Calibration focus\n",
    "\n",
    "In clinical use, a calibrated model lets you set thresholds that align with safety targets. Let's log binned reliability tables and summary scores to Weights & Biases for Validation and Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration for the current Random Forest baseline\n",
    "# Logs reliability tables and summary scores to Weights & Biases\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"calibration\",\n",
    "    config={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"seed\": SEED,\n",
    "        \"calibration_bins\": 10,\n",
    "        \"calibration_splits\": [\"val\", \"test\"]\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "\n",
    "# Helper to build a reliability table using equal-frequency bins\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    # Rank-based bins for stable counts\n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    agg = dfb.groupby(\"bin\").agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),\n",
    "        observed_rate=(\"y_true\", \"mean\"),\n",
    "        count=(\"y_true\", \"size\")\n",
    "    ).reset_index()\n",
    "    # expose numeric bin edges for plotting in W&B\n",
    "    agg[\"bin_low\"] = agg[\"bin\"].apply(lambda x: float(x.left))\n",
    "    agg[\"bin_high\"] = agg[\"bin\"].apply(lambda x: float(x.right))\n",
    "    agg = agg.drop(columns=[\"bin\"])\n",
    "    # Expected Calibration Error with equal-frequency bins\n",
    "    # Weighted by bin count over total\n",
    "    weights = agg[\"count\"] / agg[\"count\"].sum()\n",
    "    ece = float(np.sum(weights * np.abs(agg[\"observed_rate\"] - agg[\"mean_prob\"])))\n",
    "    return agg, ece\n",
    "\n",
    "# Build tables and summary scores\n",
    "cal_bins = 10\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob, n_bins=cal_bins)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob, n_bins=cal_bins)\n",
    "\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "# Log metrics\n",
    "wandb.log({\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_brier\": test_brier,\n",
    "    \"val_ece\": val_ece,\n",
    "    \"test_ece\": test_ece\n",
    "})\n",
    "\n",
    "# Log tables for interactive calibration plots in W&B\n",
    "wandb.log({\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088e269",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "- Use the calibration tables in Weights & Biases to build line or bar charts  \n",
    "- Pay special attention to the high risk bins where clinical actions concentrate  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7a0c4",
   "metadata": {},
   "source": [
    "# 8. Threshold selection for clinical use\n",
    "- We'll choose operating thresholds on Validation to hit target sensitivity and specificity  \n",
    "- We'll freeze those thresholds and evaluate on Test  \n",
    "- Finally we'll log confusion matrices and clinical metrics to Weights & Biases for easy \n",
    "\n",
    "### Remember:\n",
    "- Pick thresholds on Validation to meet clinical goals, then freeze and report Test performance  \n",
    "- Sensitivity-first thresholds catch more true cases but raise alerts, specificity-first thresholds reduce false alarms  \n",
    "- Use the Weights & Biases tables to compare predictive values and confusion matrices for each operating point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Threshold selection for the current Random Forest baseline\n",
    "\n",
    "# Compute probabilities from the already-fitted Random Forest\n",
    "y_val_prob = rf.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = rf.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"threshold_selection\",\n",
    "    config={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"seed\": SEED,\n",
    "        \"target_sensitivity\": 0.85,   # to be adjusted and defined per clinical use case\n",
    "        \"target_specificity\": 0.90    # to be adjusted and defined per clinical use case\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "# Helper: compute metrics at a given threshold\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan   # recall for positives\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan   # recall for negatives\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan           # precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "    prevalence = (tp + fn) / (tp + tn + fp + fn)\n",
    "    return dict(\n",
    "        threshold=float(thr),\n",
    "        tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sensitivity),\n",
    "        specificity=float(specificity),\n",
    "        ppv=float(ppv), npv=float(npv),\n",
    "        prevalence=float(prevalence)\n",
    "    )\n",
    "\n",
    "# Sweep a dense grid of thresholds on Validation\n",
    "grid = np.unique(np.quantile(y_val_prob, q=np.linspace(0, 1, 501)))  # 0.2 percent steps by quantiles\n",
    "\n",
    "val_rows = [metrics_at_threshold(y_val, y_val_prob, thr) for thr in grid]\n",
    "val_tbl = pd.DataFrame(val_rows)\n",
    "\n",
    "# Pick thresholds closest to targets on Validation\n",
    "t_sens = run.config[\"target_sensitivity\"]\n",
    "t_spec = run.config[\"target_specificity\"]\n",
    "\n",
    "thr_for_sens = val_tbl.iloc[(val_tbl[\"sensitivity\"] - t_sens).abs().argsort()].iloc[0][\"threshold\"]\n",
    "thr_for_spec = val_tbl.iloc[(val_tbl[\"specificity\"] - t_spec).abs().argsort()].iloc[0][\"threshold\"]\n",
    "\n",
    "# Log chosen thresholds\n",
    "wandb.config.update({\n",
    "    \"chosen_threshold_sensitivity\": float(thr_for_sens),\n",
    "    \"chosen_threshold_specificity\": float(thr_for_spec)\n",
    "}, allow_val_change=True)\n",
    "\n",
    "# Apply frozen thresholds on Test\n",
    "test_at_sens = metrics_at_threshold(y_test, y_test_prob, thr_for_sens)\n",
    "test_at_spec = metrics_at_threshold(y_test, y_test_prob, thr_for_spec)\n",
    "\n",
    "# Build W&B Tables\n",
    "val_table_wb = wandb.Table(dataframe=val_tbl[[\"threshold\",\"sensitivity\",\"specificity\",\"ppv\",\"npv\",\"prevalence\"]])\n",
    "wandb.log({\"validation_threshold_sweep\": val_table_wb})\n",
    "\n",
    "test_results_tbl = pd.DataFrame([\n",
    "    dict(target=\"sensitivity\", **test_at_sens),\n",
    "    dict(target=\"specificity\", **test_at_spec)\n",
    "])\n",
    "\n",
    "wandb.log({\n",
    "    \"test_operating_points\": wandb.Table(dataframe=test_results_tbl[[\n",
    "        \"target\",\"threshold\",\"tp\",\"fp\",\"tn\",\"fn\",\"sensitivity\",\"specificity\",\"ppv\",\"npv\",\"prevalence\"\n",
    "    ]])\n",
    "})\n",
    "\n",
    "# Also log the two confusion matrices as compact tables\n",
    "def cm_table(row):\n",
    "    return wandb.Table(data=[\n",
    "        [\"Actual 0\", row[\"tn\"], row[\"fp\"]],\n",
    "        [\"Actual 1\", row[\"fn\"], row[\"tp\"]],\n",
    "    ], columns=[\"\", \"Pred 0\", \"Pred 1\"])\n",
    "\n",
    "wandb.log({\n",
    "    \"confusion_matrix_test_at_sensitivity\": cm_table(test_at_sens),\n",
    "    \"confusion_matrix_test_at_specificity\": cm_table(test_at_spec)\n",
    "})\n",
    "\n",
    "print(f\"Chosen thresholds -> Sensitivity target: {thr_for_sens:.3f}, Specificity target: {thr_for_spec:.3f}\")\n",
    "print(\"Test metrics at sensitivity target:\", {k: v for k, v in test_at_sens.items() if k not in [\"tp\",\"fp\",\"tn\",\"fn\"]})\n",
    "print(\"Test metrics at specificity target:\", {k: v for k, v in test_at_spec.items() if k not in [\"tp\",\"fp\",\"tn\",\"fn\"]})\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e07314",
   "metadata": {},
   "source": [
    "# 9. Subgroup performance\n",
    "\n",
    "We'll evaluate the Random Forest on clinically relevant subgroups to check consistency of performance\n",
    "Subgroups (as identified during our CPH analysis):\n",
    "- **SOFA** score (severity of illness)\n",
    "- **CSRU** (Cardiac Surgery Recovery Unit)\n",
    "\n",
    "These two variables were the most significant predictors of mortality in the Cox Proportional Hazards analysis.  \n",
    "We will use the thresholds chosen on Validation and report metrics on Test in Weights & Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c772353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroup performance on Test using SOFA bands and CSRU flag\n",
    "# Uses thresholds thr_for_sens and thr_for_spec chosen in the previous step\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"subgroup_eval\",\n",
    "    config={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"seed\": SEED,\n",
    "        \"subgroups\": [\"SOFA_bin\", \"CSRU\"]\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "# Define subgroup columns\n",
    "SOFA_COL = \"SOFA\"\n",
    "CSRU_COL = \"CSRU\"\n",
    "\n",
    "# Extract subgroups from Test set\n",
    "test_idx = X_test.index\n",
    "sofa_test = ICU.loc[test_idx, SOFA_COL]\n",
    "csru_test = ICU.loc[test_idx, CSRU_COL]\n",
    "\n",
    "# Create SOFA quantile bins (5 bands by severity)\n",
    "sofa_bins = pd.qcut(sofa_test, q=5, duplicates=\"drop\").astype(str)\n",
    "\n",
    "# Binary label for CSRU membership\n",
    "csru_group = np.where(pd.to_numeric(csru_test, errors=\"coerce\").fillna(0).astype(int) == 1, \"CSRU\", \"non_CSRU\")\n",
    "\n",
    "# Helper to compute metrics at fixed threshold\n",
    "def metrics_fixed_threshold(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    spec = tn / (tn + fp) if (tn + fp) else np.nan\n",
    "    ppv  = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "    npv  = tn / (tn + fn) if (tn + fn) else np.nan\n",
    "    prev = (tp + fn) / (tp + tn + fp + fn)\n",
    "    return dict(\n",
    "        tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sens), specificity=float(spec),\n",
    "        ppv=float(ppv), npv=float(npv), prevalence=float(prev)\n",
    "    )\n",
    "\n",
    "# Aggregate metrics per subgroup\n",
    "rows = []\n",
    "\n",
    "def add_group(group_name, group_values):\n",
    "    series = pd.Series(group_values, index=test_idx).astype(str)\n",
    "    for g in sorted(series.unique()):\n",
    "        mask = (series == g).values\n",
    "        y_true_g = y_test.values[mask]\n",
    "        y_prob_g = y_test_prob[mask]\n",
    "        if len(y_true_g) < 10:\n",
    "            continue\n",
    "        auroc = roc_auc_score(y_true_g, y_prob_g)\n",
    "        auprc = average_precision_score(y_true_g, y_prob_g)\n",
    "        m_sens = metrics_fixed_threshold(y_true_g, y_prob_g, thr_for_sens)\n",
    "        m_spec = metrics_fixed_threshold(y_true_g, y_prob_g, thr_for_spec)\n",
    "        rows.append({\n",
    "            \"model_type\": \"random_forest\",\n",
    "            \"subgroup_type\": group_name,\n",
    "            \"subgroup_value\": g,\n",
    "            \"n\": int(len(y_true_g)),\n",
    "            \"auroc\": float(auroc),\n",
    "            \"auprc\": float(auprc),\n",
    "            \"target\": \"sensitivity\",\n",
    "            \"threshold\": float(thr_for_sens),\n",
    "            **m_sens\n",
    "        })\n",
    "        rows.append({\n",
    "            \"model_type\": \"random_forest\",\n",
    "            \"subgroup_type\": group_name,\n",
    "            \"subgroup_value\": g,\n",
    "            \"n\": int(len(y_true_g)),\n",
    "            \"auroc\": float(auroc),\n",
    "            \"auprc\": float(auprc),\n",
    "            \"target\": \"specificity\",\n",
    "            \"threshold\": float(thr_for_spec),\n",
    "            **m_spec\n",
    "        })\n",
    "\n",
    "# Add SOFA and CSRU subgroups\n",
    "add_group(\"SOFA_bin\", sofa_bins)\n",
    "add_group(\"ICU_unit\", csru_group)\n",
    "\n",
    "subgroup_df = pd.DataFrame(rows)\n",
    "\n",
    "wandb.log({\n",
    "    \"subgroup_metrics_test\": wandb.Table(dataframe=subgroup_df[[\n",
    "        \"model_type\",\"subgroup_type\",\"subgroup_value\",\"n\",\n",
    "        \"target\",\"threshold\",\n",
    "        \"auroc\",\"auprc\",\"sensitivity\",\"specificity\",\"ppv\",\"npv\",\"prevalence\",\n",
    "        \"tp\",\"fp\",\"tn\",\"fn\"\n",
    "    ]]),\n",
    "    \"subgroup_columns_used\": wandb.Table(data=[[SOFA_COL, CSRU_COL]], columns=[\"sofa_column\",\"icu_unit_column\"])\n",
    "})\n",
    "\n",
    "print(\"Logged subgroup metrics for SOFA bins and CSRU vs non-CSRU on Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2861bb",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "- In the Weights & Biases table, compare SOFA bins and CSRU vs non CSRU\n",
    "- Look for drops in sensitivity or PPV at the chosen threshold\n",
    "- If performance varies widely by subgroup, discuss mitigation options such as recalibration or separate thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195184a",
   "metadata": {},
   "source": [
    "# 10. Interpretability for understanding\n",
    "\n",
    "Model interpretability connects predictive performance to clinical meaning:\n",
    "- Use **Permutation Importance** for Logistic Regression, Decision Tree, and Random Forest  \n",
    "- Use **SHAP** on the Random Forest to see which features drive individual predictions  \n",
    "\n",
    "All outputs are logged to Weights & Biases for exploration and comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability: Permutation Importance and SHAP for Random Forest\n",
    "# Logs all interpretability outputs to W&B\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"interpretability\",\n",
    "    config={\n",
    "        \"models\": [\"logistic_regression\", \"decision_tree\", \"random_forest\"],\n",
    "        \"shap_sample_size\": 500\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "# 1) Permutation Importance for all models on Validation\n",
    "models = {\n",
    "    \"logistic_regression\": log_reg,\n",
    "    \"decision_tree\": dt,\n",
    "    \"random_forest\": rf\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    result = permutation_importance(\n",
    "        model, X_val_t, y_val, n_repeats=10, random_state=SEED, n_jobs=-1\n",
    "    )\n",
    "    imp_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X_val_t.columns,\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std\n",
    "        })\n",
    "        .sort_values(\"importance_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    wandb.log({f\"{model_name}_permutation_importance\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# 2) SHAP for Random Forest on a small Validation sample\n",
    "# Use model_output=\"raw\" for tree_path_dependent\n",
    "# Log summary plot image and a ranked table of mean |SHAP| values\n",
    "\n",
    "\n",
    "# Sample a manageable slice from Validation\n",
    "shap_sample_n = min(500, len(X_val_t))\n",
    "shap_sample = X_val_t.sample(n=shap_sample_n, random_state=SEED)\n",
    "\n",
    "# Use raw output with tree_path_dependent for tree models\n",
    "explainer = shap.TreeExplainer(\n",
    "    rf,\n",
    "    model_output=\"raw\",\n",
    "    feature_perturbation=\"tree_path_dependent\"\n",
    ")\n",
    "\n",
    "# Avoid additivity mismatches across sklearn versions\n",
    "shap_values_raw = explainer.shap_values(shap_sample, check_additivity=False)\n",
    "\n",
    "# Select SHAP values for the positive class and ensure 2D shape (n_samples, n_features)\n",
    "if isinstance(shap_values_raw, list):\n",
    "    sv = np.asarray(shap_values_raw[1])  # class 1\n",
    "else:\n",
    "    sv = np.asarray(shap_values_raw)\n",
    "\n",
    "# If 3D, last axis usually indexes classes\n",
    "if sv.ndim == 3:\n",
    "    # prefer class 1 if available, else class 0\n",
    "    cls_axis = sv.shape[-1]\n",
    "    pick = 1 if cls_axis >= 2 else 0\n",
    "    sv = sv[..., pick]\n",
    "\n",
    "# Safety: squeeze any trailing singleton dims\n",
    "sv = np.squeeze(sv)\n",
    "assert sv.ndim == 2, f\"Expected 2D SHAP values after selection, got {sv.shape}\"\n",
    "assert sv.shape[1] == shap_sample.shape[1], \"Feature count mismatch between SHAP and input data\"\n",
    "\n",
    "# Global summary plot\n",
    "shap.summary_plot(sv, shap_sample, show=False)\n",
    "wandb.log({\"shap_summary_plot\": wandb.Image(plt.gcf())})\n",
    "plt.close()\n",
    "\n",
    "# Ranked mean absolute SHAP for table\n",
    "mean_abs_shap = np.abs(sv).mean(axis=0).reshape(-1)\n",
    "feat_names = list(shap_sample.columns)\n",
    "\n",
    "shap_df = pd.DataFrame(\n",
    "    {\"feature\": feat_names, \"mean_abs_shap\": mean_abs_shap}\n",
    ").sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "wandb.log({\"rf_shap_feature_importance\": wandb.Table(dataframe=shap_df)})\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cf119",
   "metadata": {},
   "source": [
    "### To DO\n",
    "In Weights & Biases:\n",
    "- Use the permutation importance tables to see which features most influence predictions  \n",
    "- Compare across models to note stability of top features  \n",
    "- In the SHAP summary, focus on high impact variables — for instance, rising SOFA and CSRU membership should drive higher predicted mortality  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d902532",
   "metadata": {},
   "source": [
    "# 11. Random Forest hyperparameter sweep\n",
    "\n",
    "We will run a short Weights & Biases Sweep to tune Random Forest hyperparameters with the goal if **Maximizing validation area under the Precision-Recall curve** \n",
    "- REmmeber Precision-Recall is more informative than ROC under class imbalance\n",
    "\n",
    "We log validation AUROC and Brier score as secondary signals for discrimination and calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest sweep optimized for imbalanced data using validation AUPRC\n",
    "# Logs val_pr as the sweep objective, plus val_auc and val_brier for context\n",
    "\n",
    "# Sweep training function\n",
    "def train_rf_sweep():\n",
    "    run = wandb.init(project=WB_PROJECT, job_type=\"rf_sweep\", reinit=True, settings=wandb.Settings(start_method=\"thread\"))\n",
    "    cfg = wandb.config\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=cfg.n_estimators,\n",
    "        max_depth=None if cfg.max_depth == 0 else cfg.max_depth,\n",
    "        max_features=cfg.max_features,\n",
    "        min_samples_leaf=cfg.min_samples_leaf,\n",
    "        class_weight=\"balanced\",   # encourage attention to minority class\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_t, y_train)\n",
    "\n",
    "    # Validation probabilities\n",
    "    y_val_prob = model.predict_proba(X_val_t)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    val_pr   = average_precision_score(y_val, y_val_prob)     # sweep objective\n",
    "    val_auc  = roc_auc_score(y_val, y_val_prob)\n",
    "    val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        \"val_pr\": val_pr,\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_brier\": val_brier\n",
    "    })\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "# Compact search space\n",
    "sweep_config = {\n",
    "    \"name\": \"rf_pr_tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_pr\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"n_estimators\": {\"values\": [100, 200, 300, 500]},\n",
    "        \"max_depth\": {\"values\": [0, 8, 12, 16]},          # 0 means None\n",
    "        \"max_features\": {\"values\": [\"sqrt\", \"log2\"]},\n",
    "        \"min_samples_leaf\": {\"values\": [1, 3, 5, 10]}\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Launch the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=WB_PROJECT)\n",
    "wandb.agent(sweep_id, function=train_rf_sweep, count=5)\n",
    "\n",
    "print(\"Sweep complete. In Weights & Biases, sort by val_pr and inspect Parameter Importance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd112b1d",
   "metadata": {},
   "source": [
    "### to Do\n",
    "\n",
    "- Use the Sweep table to sort by validation area under PR and review parameter importance\n",
    "- Confirm that gains in PR do not severely harm calibration or AUROC\n",
    "- Select the best configuration for final refit and Test evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b646d44",
   "metadata": {},
   "source": [
    "# 12. Best model selection and test evaluation\n",
    "\n",
    "Let's pick the Random Forest configuration with the best validation Precision-Recall area from the sweep. Then we refit that model on Train + Validation and evaluate on Test.\n",
    "- All final metrics and calibration data are logged to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aae77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Random Forest evaluation on Test using best sweep configuration\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"final_eval\",\n",
    "    config={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"train_split\": \"train+val\",\n",
    "        \"test_split\": \"test\",\n",
    "        \"objective\": \"maximize_val_pr\"\n",
    "    },\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "# Example best parameters from sweep — replace with the actual top run's config\n",
    "best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 12,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"min_samples_leaf\": 3,\n",
    "    \"class_weight\": \"balanced\"\n",
    "}\n",
    "\n",
    "wandb.config.update(best_params, allow_val_change=True)\n",
    "\n",
    "# Combine Train + Validation for final training\n",
    "X_train_full_t = pd.concat([X_train_t, X_val_t])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    class_weight=best_params[\"class_weight\"],\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_best.fit(X_train_full_t, y_train_full)\n",
    "\n",
    "# Predictions on Test\n",
    "y_test_prob = rf_best.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "test_pr = average_precision_score(y_test, y_test_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# Calibration table for reliability curve\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    agg = dfb.groupby(\"bin\").agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),\n",
    "        observed_rate=(\"y_true\", \"mean\"),\n",
    "        count=(\"y_true\", \"size\")\n",
    "    ).reset_index(drop=True)\n",
    "    return agg\n",
    "\n",
    "cal_test_tbl = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\"calibration_table_test_final\": wandb.Table(dataframe=cal_test_tbl)})\n",
    "\n",
    "# Predictions table for slicing and auditability\n",
    "pred_tbl = pd.DataFrame({\n",
    "    \"id\": X_test.index,\n",
    "    \"y_true\": y_test.values,\n",
    "    \"y_prob\": y_test_prob\n",
    "})\n",
    "wandb.log({\"final_test_predictions\": wandb.Table(dataframe=pred_tbl)})\n",
    "\n",
    "print(f\"Final Test AUROC {test_auc:.3f}, AUPRC {test_pr:.3f}, Brier {test_brier:.3f}\")\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536e19b",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "- Use the final Weights & Biases run to confirm the tuned model’s discrimination and calibration  \n",
    "- Compare its Test AUPRC to the baseline Random Forest  \n",
    "- Check the calibration table for reliability and ensure predictions align with expected mortality probabilities  \n",
    "- This final model closes the loop between data preparation, model tuning, and clinical interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8c091",
   "metadata": {},
   "source": [
    "# 14. Model documentation and governance\n",
    "\n",
    "Every clinically oriented ML model must have transparent documentation (README.md, incode doc, markdowns, etc.). We will log a short model card into Weights & Biases describing:\n",
    "- Intended use\n",
    "- Data provenance and cohort\n",
    "- Known limitations and fairness notes\n",
    "- Regulatory traceability elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07dc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight model card for transparency and traceability\n",
    "# Logged as a W&B summary field so it travels with the model\n",
    "\n",
    "model_card = \"\"\"\n",
    "**Model:** Random Forest (final tuned)\n",
    "**Objective:** Predict in-hospital mortality for ICU patients\n",
    "**Dataset:** PhysioNet 2012 (Set A)\n",
    "**Key subgroups:** SOFA score, CSRU vs non-CSRU\n",
    "**Calibration:** Verified on Test with reliability tables\n",
    "**Thresholds:** Fixed on Validation (sensitivity ≈ 0.85, specificity ≈ 0.90)\n",
    "**Intended use:** Decision-support to flag high-risk patients, not for autonomous triage\n",
    "**Known limitations:**\n",
    "- Derived from retrospective data, not externally validated\n",
    "- Potential bias if SOFA or CSRU distribution shifts\n",
    "- Performance not guaranteed outside adult ICU cohort\n",
    "**Governance:**\n",
    "- All experiments tracked in W&B for audit\n",
    "- Code and preprocessing pipelines versioned in this notebook\n",
    "- Model retraining required if data schema changes\n",
    "\"\"\"\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"documentation\",\n",
    "    config={\"model_type\": \"random_forest\", \"documentation\": \"model_card\"},\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "wandb.summary[\"model_card\"] = model_card\n",
    "run.finish()\n",
    "\n",
    "print(\"Model card logged to W&B summary under key 'model_card'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664a395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu-survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
