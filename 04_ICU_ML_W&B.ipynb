{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6df2cb",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "# ML for Healthcare with Weights & Biases: Interpretable Baselines and Clinical Evaluation\n",
    "\n",
    "We continue our focus on shifting from coding models to *understanding* them — how well they perform, how trustworthy their probabilities are, and how they behave across patient groups. Strenghtening the foundation for clinically aware ML practice.\n",
    "\n",
    "**Objective**  \n",
    "Build, track, and interpret models predicting in-hospital mortality in ICU patients using Weights & Biases (W&B).  \n",
    "This notebook transforms standard Machine Learning (ML) practice into an auditable, interpretable, and clinically meaningful workflow\n",
    "\n",
    "**You will learn**\n",
    "- How to track model training and evaluation runs using Weights & Biases  \n",
    "- How to interpret models and understand their calibration  \n",
    "- How to examine fairness and subgroup performance  \n",
    "- How to communicate model insights for healthcare decisions\n",
    "\n",
    "**Models**\n",
    "We’ll compare three interpretable baselines:\n",
    "1. **Logistic Regression**: simple linear reference, easy to explain  \n",
    "2. **Decision Tree (shallow)**: intuitive splits, visually transparent  \n",
    "3. **Random Forest**: robust ensemble, main focus for tuning and interpretability\n",
    "\n",
    "**Dataset**\n",
    "[PhysioNet Challenge 2012 dataset](https://physionet.org/content/challenge-2012/1.0.0/), containing clinical measurements, demographics, and the target:  \n",
    "`In-hospital_death` (binary: 1 = patient died during stay, 0 = survived).\n",
    "\n",
    "**Weights & Biases**\n",
    "Used to:\n",
    "- Track configurations, metrics, and plots  \n",
    "- Compare models and hyperparameters  \n",
    "- Log interpretability results (feature importance, calibration, subgroups)  \n",
    "- Support model transparency and documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201233",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca632bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idiaz\\miniconda3\\envs\\icu-survival\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "c:\\Users\\idiaz\\miniconda3\\envs\\icu-survival\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports, reproducibility, and Weights & Biases initialization\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import joblib # Added for model saving\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "import wandb\n",
    "from wandb.plot import roc_curve as wandb_roc_curve\n",
    "from wandb.plot import pr_curve as wandb_pr_curve\n",
    "from wandb.plot import confusion_matrix as wandb_cm\n",
    "from wandb import Api as WandbApi # Added for sweep automation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    average_precision_score, \n",
    "    brier_score_loss,\n",
    "    roc_curve, \n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login\n",
    "# !wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4e94e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: idiazl to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\idiaz\\OneDrive - IE University\\00. IE Courses\\01. 2025_H2\\1. ML4HC\\ICU_Survival_analysis\\wandb\\run-20251023_104007-xopxd1me</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/xopxd1me' target=\"_blank\">01-data-exploration</a></strong> to <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/xopxd1me' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/xopxd1me</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights & Biases tracking URL: https://wandb.ai/idiazl/ml-healthcare-intro/runs/xopxd1me\n"
     ]
    }
   ],
   "source": [
    "WB_PROJECT = \"ml-healthcare-intro\"\n",
    "\n",
    "# wandb.login() # Uncomment if not logged in\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT, \n",
    "    job_type=\"data-exploration\", # More descriptive job_type\n",
    "    name=\"01-data-exploration\",  # Clean name for the UI\n",
    "    config={\n",
    "        \"seed\": SEED,\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"dataset\": \"physionet2012_set_a\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log environment metadata\n",
    "wandb.config.update({\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"pandas_version\": pd.__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "}, allow_val_change=True)\n",
    "\n",
    "print(f\"Weights & Biases tracking URL: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ffd0e",
   "metadata": {},
   "source": [
    "# 3. Data loading and initial checks\n",
    "We will load the dataset, confirm the target, and log basic summaries to Weights & Biases. This lets us explore class balance, missingness, and a quick data preview directly in the dashboard.\n",
    "\n",
    "### Action Items\n",
    "- Open the W&B run link generated above\n",
    "- Inspect the `data_preview_table` to understand the features\n",
    "- Check the `class_balance_table` to confirm the target imbalance\n",
    "- Review `missingness_top30_table` to identify problematic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea8f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ICU with shape (4000, 120) and positive rate 0.139\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "PATH = \"PhysionetChallenge2012-set-a.csv.gz\"\n",
    "\n",
    "# Simple check to ensure the data file exists before trying to load it\n",
    "if not os.path.exists(PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Error: The data file was not found at '{PATH}'. \"\n",
    "        \"Please ensure the dataset is in the correct directory.\"\n",
    "    )\n",
    "\n",
    "ICU = pd.read_csv(PATH, compression=\"gzip\")\n",
    "\n",
    "TARGET = \"In-hospital_death\"\n",
    "ID_COL = \"recordid\" if \"recordid\" in ICU.columns else None\n",
    "\n",
    "if TARGET not in ICU.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in dataset\")\n",
    "\n",
    "# Ensure target is numeric and binary\n",
    "ICU[TARGET] = pd.to_numeric(ICU[TARGET], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Basic facts\n",
    "n_rows, n_cols = ICU.shape\n",
    "pos_rate = float(ICU[TARGET].mean())\n",
    "\n",
    "# Class balance table\n",
    "cb_series = ICU[TARGET].value_counts().sort_index()\n",
    "class_balance_tbl = wandb.Table(data=[[int(k), int(v), float(v / n_rows)] for k, v in cb_series.items()],\n",
    "                                columns=[\"label\", \"count\", \"fraction\"])\n",
    "\n",
    "# Missingness table (top 30)\n",
    "miss = ICU.isna().mean().sort_values(ascending=False)\n",
    "miss_top = miss.head(30).reset_index()\n",
    "miss_top.columns = [\"column\", \"missing_fraction\"]\n",
    "missing_tbl = wandb.Table(data=miss_top.values.tolist(), columns=list(miss_top.columns))\n",
    "\n",
    "# Data preview table (sample up to 200 rows for UI responsiveness)\n",
    "preview = ICU.sample(n=min(200, len(ICU)), random_state=SEED)\n",
    "preview_tbl = wandb.Table(dataframe=preview)\n",
    "\n",
    "wandb.log({\n",
    "    \"dataset_rows\": n_rows,\n",
    "    \"dataset_cols\": n_cols,\n",
    "    \"positive_rate\": pos_rate,\n",
    "    \"class_balance_table\": class_balance_tbl,\n",
    "    \"missingness_top30_table\": missing_tbl,\n",
    "    \"data_preview_table\": preview_tbl\n",
    "})\n",
    "\n",
    "print(f\"Loaded ICU with shape {ICU.shape} and positive rate {pos_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68fdbf",
   "metadata": {},
   "source": [
    "# 4. Simple preprocessing\n",
    "### Preprocessing and splits\n",
    "We will split the data into train, validation, and test sets, impute missing values and one-hot encode categorical variables, and log feature lists and split sizes to Weights & Biases for transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04bc083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2400, 118), Val: (800, 118), Test: (800, 118)\n",
      "Preprocessing complete\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset_cols</td><td>▁</td></tr><tr><td>dataset_rows</td><td>▁</td></tr><tr><td>n_cat_features_raw</td><td>▁</td></tr><tr><td>n_features_transformed</td><td>▁</td></tr><tr><td>n_num_features_raw</td><td>▁</td></tr><tr><td>positive_rate</td><td>▁</td></tr><tr><td>test_rows</td><td>▁</td></tr><tr><td>train_rows</td><td>▁</td></tr><tr><td>val_rows</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset_cols</td><td>120</td></tr><tr><td>dataset_rows</td><td>4000</td></tr><tr><td>n_cat_features_raw</td><td>0</td></tr><tr><td>n_features_transformed</td><td>118</td></tr><tr><td>n_num_features_raw</td><td>118</td></tr><tr><td>positive_rate</td><td>0.1385</td></tr><tr><td>test_rows</td><td>800</td></tr><tr><td>train_rows</td><td>2400</td></tr><tr><td>val_rows</td><td>800</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">01-data-exploration</strong> at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/xopxd1me' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/xopxd1me</a><br> View project at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a><br>Synced 5 W&B file(s), 5 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251023_104007-xopxd1me\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing: split data and prepare simple pipelines\n",
    "\n",
    "# Drop ID column if present\n",
    "X = ICU.drop(columns=[c for c in [TARGET, ID_COL] if c in ICU.columns])\n",
    "y = ICU[TARGET]\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# Split data (60 percent train, 20 percent validation, 20 percent test)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, stratify=y_train_full, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Define transformations\n",
    "num_transformer = SimpleImputer(strategy=\"median\")\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, num_cols),\n",
    "        (\"cat\", cat_transformer, cat_cols)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "# Log split sizes\n",
    "wandb.log({\n",
    "    \"train_rows\": len(X_train),\n",
    "    \"val_rows\": len(X_val),\n",
    "    \"test_rows\": len(X_test),\n",
    "    \"n_num_features_raw\": len(num_cols),\n",
    "    \"n_cat_features_raw\": len(cat_cols),\n",
    "    \"n_features_transformed\": X_train_t.shape[1]\n",
    "})\n",
    "\n",
    "# Log feature lists as W&B Tables for inspectability\n",
    "num_tbl = wandb.Table(data=[[c, \"numeric\"] for c in num_cols], columns=[\"feature\", \"type\"])\n",
    "cat_tbl = wandb.Table(data=[[c, \"categorical\"] for c in cat_cols], columns=[\"feature\", \"type\"])\n",
    "wandb.log({\"feature_list_numeric\": num_tbl, \"feature_list_categorical\": cat_tbl})\n",
    "\n",
    "print(\"Preprocessing complete\")\n",
    "\n",
    "# Finish the data exploration run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01940624",
   "metadata": {},
   "source": [
    "Use the feature tables and split sizes in Weights & Biases to verify preprocessing choices  \n",
    "All models next will consume the same transformed matrices for fair comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6087c95",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression: establishing a simple reference\n",
    "\n",
    "Before exploring complex models, it’s helpful to start with a simple and interpretable baseline. Logistic Regression gives a linear relationship between features and the log-odds of the outcome. Helping us understand whether more flexible models (like Random Forests) truly add value.\n",
    "\n",
    "1. We’ll train a Logistic Regression model, evaluate it on the validation and test sets\n",
    "2. Log all metrics to Weights & Biases to compare later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8328bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\idiaz\\OneDrive - IE University\\00. IE Courses\\01. 2025_H2\\1. ML4HC\\ICU_Survival_analysis\\wandb\\run-20251023_104121-6g37one6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/6g37one6' target=\"_blank\">02-baseline-logistic-regression</a></strong> to <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/6g37one6' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/6g37one6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_brier</td><td>▁</td></tr><tr><td>test_ece</td><td>▁</td></tr><tr><td>test_pr</td><td>▁</td></tr><tr><td>val_auc</td><td>▁</td></tr><tr><td>val_brier</td><td>▁</td></tr><tr><td>val_ece</td><td>▁</td></tr><tr><td>val_pr</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>0.8622</td></tr><tr><td>test_brier</td><td>0.09375</td></tr><tr><td>test_ece</td><td>0.0338</td></tr><tr><td>test_pr</td><td>0.4755</td></tr><tr><td>val_auc</td><td>0.87606</td></tr><tr><td>val_brier</td><td>0.08569</td></tr><tr><td>val_ece</td><td>0.02047</td></tr><tr><td>val_pr</td><td>0.55215</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">02-baseline-logistic-regression</strong> at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/6g37one6' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/6g37one6</a><br> View project at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251023_104121-6g37one6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR validation AUROC 0.876, AUPRC 0.552, Brier 0.086, ECE 0.020\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression baseline with W&B's built-in plotting\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    name=\"02-baseline-logistic-regression\", # Add clean name\n",
    "    config={\"model_type\": \"logistic_regression\", \"seed\": SEED},\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# --- Helper function for calibration ---\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    agg = dfb.groupby(\"bin\", observed=False).agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),\n",
    "        observed_rate=(\"y_true\", \"mean\"),\n",
    "        count=(\"y_true\", \"size\")\n",
    "    ).reset_index()\n",
    "    agg[\"bin\"] = agg[\"bin\"].astype(str)\n",
    "    weights = agg[\"count\"] / agg[\"count\"].sum()\n",
    "    ece = float(np.sum(weights * np.abs(agg[\"observed_rate\"] - agg[\"mean_prob\"])))\n",
    "    return agg, ece\n",
    "# --- End Helper ---\n",
    "\n",
    "# Train\n",
    "log_reg = LogisticRegression(max_iter=500, solver=\"liblinear\", random_state=SEED)\n",
    "log_reg.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = log_reg.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = log_reg.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# --- Log Calibration Metrics ---\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\n",
    "    \"val_ece\": val_ece, \"test_ece\": test_ece,\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "\n",
    "# Create the 2D probability array that wandb.plot expects\n",
    "y_val_probas_2d = np.stack([1.0 - y_val_prob, y_val_prob], axis=1)\n",
    "\n",
    "# Pass the 2D array and remove the 'labels' argument\n",
    "wandb.log({\n",
    "    \"roc_curve_val\": wandb_roc_curve(y_val.values, y_val_probas_2d),\n",
    "    \"pr_curve_val\": wandb_pr_curve(y_val.values, y_val_probas_2d)\n",
    "})\n",
    "\n",
    "# Coefficients for transparency\n",
    "coef_df = pd.DataFrame({\"feature\": X_train_t.columns, \"coefficient\": log_reg.coef_[0]})\n",
    "coef_tbl = wandb.Table(dataframe=coef_df.sort_values(\"coefficient\", ascending=False))\n",
    "wandb.log({\"log_reg_coefficients\": coef_tbl})\n",
    "\n",
    "# Predictions table sample\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index if ID_COL is None else X_val.index, \n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"LR validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}, ECE {val_ece:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b057b",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- Open the new \"02-baseline-logistic-regression\" run in W&B\n",
    "- Examine the interactive `roc_curve_val` and `pr_curve_val` plots\n",
    "- Sort the `log_reg_coefficients` table to find the strongest predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859adf44",
   "metadata": {},
   "source": [
    "# 6. Decision Tree baseline\n",
    "\n",
    "A shallow tree is easy to read and helps us see simple non-linear rules. We will train a small tree, log metrics, curve points, feature importances, and a predictions sample to Weights & Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13c561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\idiaz\\OneDrive - IE University\\00. IE Courses\\01. 2025_H2\\1. ML4HC\\ICU_Survival_analysis\\wandb\\run-20251023_104143-y8pznvlc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/y8pznvlc' target=\"_blank\">02-baseline-decision-tree</a></strong> to <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/y8pznvlc' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/y8pznvlc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_brier</td><td>▁</td></tr><tr><td>test_ece</td><td>▁</td></tr><tr><td>test_pr</td><td>▁</td></tr><tr><td>val_auc</td><td>▁</td></tr><tr><td>val_brier</td><td>▁</td></tr><tr><td>val_ece</td><td>▁</td></tr><tr><td>val_pr</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>0.99098</td></tr><tr><td>test_brier</td><td>0.01047</td></tr><tr><td>test_ece</td><td>0.00647</td></tr><tr><td>test_pr</td><td>0.94055</td></tr><tr><td>val_auc</td><td>0.98514</td></tr><tr><td>val_brier</td><td>0.01532</td></tr><tr><td>val_ece</td><td>0.00674</td></tr><tr><td>val_pr</td><td>0.9238</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">02-baseline-decision-tree</strong> at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/y8pznvlc' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/y8pznvlc</a><br> View project at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251023_104143-y8pznvlc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT validation AUROC 0.985, AUPRC 0.924, Brier 0.015, ECE 0.007\n"
     ]
    }
   ],
   "source": [
    "# 6. Decision Tree baseline\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    name=\"02-baseline-decision-tree\", # Add clean name\n",
    "    config={\n",
    "        \"model_type\": \"decision_tree\",\n",
    "        \"seed\": SEED,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_samples_leaf\": 20\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# --- Helper function for calibration ---\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    agg = dfb.groupby(\"bin\", observed=False).agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),\n",
    "        observed_rate=(\"y_true\", \"mean\"),\n",
    "        count=(\"y_true\", \"size\")\n",
    "    ).reset_index()\n",
    "    agg[\"bin\"] = agg[\"bin\"].astype(str)\n",
    "    weights = agg[\"count\"] / agg[\"count\"].sum()\n",
    "    ece = float(np.sum(weights * np.abs(agg[\"observed_rate\"] - agg[\"mean_prob\"])))\n",
    "    return agg, ece\n",
    "# --- End Helper ---\n",
    "\n",
    "# Train a small, readable tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=wandb.config.max_depth,\n",
    "    min_samples_leaf=wandb.config.min_samples_leaf,\n",
    "    random_state=SEED\n",
    ")\n",
    "dt.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = dt.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = dt.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# --- Log Calibration Metrics ---\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\n",
    "    \"val_ece\": val_ece, \"test_ece\": test_ece,\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "\n",
    "# Create the 2D probability array that wandb.plot expects\n",
    "y_val_probas_2d = np.stack([1.0 - y_val_prob, y_val_prob], axis=1)\n",
    "\n",
    "# Pass the 2D array and remove the 'labels' argument\n",
    "wandb.log({\n",
    "    \"roc_curve_val\": wandb_roc_curve(y_val.values, y_val_probas_2d),\n",
    "    \"pr_curve_val\": wandb_pr_curve(y_val.values, y_val_probas_2d)\n",
    "})\n",
    "\n",
    "# Feature importances\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": X_train_t.columns, \"importance\": dt.feature_importances_})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# Predictions sample\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index,\n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"DT validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}, ECE {val_ece:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb201b",
   "metadata": {},
   "source": [
    "# 7. Random Forest baseline\n",
    "### Random Forest baseline\n",
    "\n",
    "As we know, a Random Forest averages many trees to improve stability and performance. We will train a baseline model and log metrics, curve points, feature importances, and a predictions sample to W&B. Later we will tune hyperparameters with a short sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Random Forest Comprehensive Baseline Analysis\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline-comprehensive\", # New job_type\n",
    "    name=\"02-baseline-random-forest-full\", # New clean name\n",
    "    config={\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"seed\": SEED,\n",
    "        \"n_estimators\": 300,\n",
    "        \"max_depth\": None,\n",
    "        \"max_features\": \"sqrt\",\n",
    "        \"min_samples_leaf\": 5,\n",
    "        \"n_jobs\": -1,\n",
    "        # Add config for calibration and thresholding\n",
    "        \"calibration_bins\": 10,\n",
    "        \"target_sensitivity\": 0.85,\n",
    "        \"target_specificity\": 0.90,\n",
    "        \"subgroups\": [\"SOFA_bin\", \"CSRU\"]\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# --- 1. Train Model ---\n",
    "cfg = wandb.config # Use config object\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=cfg.n_estimators,\n",
    "    max_depth=cfg.max_depth,\n",
    "    max_features=cfg.max_features,\n",
    "    min_samples_leaf=cfg.min_samples_leaf,\n",
    "    random_state=SEED,\n",
    "    n_jobs=cfg.n_jobs\n",
    ")\n",
    "rf.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = rf.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = rf.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# --- 2. Log Baseline Metrics & Plots ---\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "metrics_log = {\n",
    "    \"val_auc\": val_auc, \"val_pr\": val_pr, \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc, \"test_pr\": test_pr, \"test_brier\": test_brier\n",
    "}\n",
    "wandb.log(metrics_log)\n",
    "\n",
    "# Create the 2D probability array that wandb.plot expects\n",
    "y_val_probas_2d = np.stack([1.0 - y_val_prob, y_val_prob], axis=1)\n",
    "\n",
    "# Pass the 2D array and remove the 'labels' argument\n",
    "wandb.log({\n",
    "    \"roc_curve_val\": wandb_roc_curve(y_val.values, y_val_probas_2d),\n",
    "    \"pr_curve_val\": wandb_pr_curve(y_val.values, y_val_probas_2d)\n",
    "})\n",
    "\n",
    "# Log feature importances\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": X_train_t.columns, \"importance\": rf.feature_importances_})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# Log predictions sample\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index, \"y_true\": y_val.values, \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "print(f\"RF validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}\")\n",
    "\n",
    "# --- 3. Calibration Analysis (from original cell 20) ---\n",
    "\n",
    "# --- CORRECTION: Modified helper function ---\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    # Ensure y_true is a numpy array if it's a pandas Series\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "        \n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    \n",
    "    agg = dfb.groupby(\"bin\", observed=False).agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),\n",
    "        observed_rate=(\"y_true\", \"mean\"),\n",
    "        count=(\"y_true\", \"size\")\n",
    "    ).reset_index()\n",
    "    \n",
    "    # --- FIX: Convert Interval 'bin' column to string ---\n",
    "    # This makes it JSON serializable for wandb.Table\n",
    "    agg[\"bin\"] = agg[\"bin\"].astype(str)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    weights = agg[\"count\"] / agg[\"count\"].sum()\n",
    "    ece = float(np.sum(weights * np.abs(agg[\"observed_rate\"] - agg[\"mean_prob\"])))\n",
    "    return agg, ece\n",
    "\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob, n_bins=cfg.calibration_bins)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob, n_bins=cfg.calibration_bins)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_ece\": val_ece, \"test_ece\": test_ece,\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "print(f\"RF validation ECE {val_ece:.3f}, Test ECE {test_ece:.3f}\")\n",
    "\n",
    "\n",
    "# --- 4. Threshold Selection (from original cell 23) ---\n",
    "\n",
    "# Helper: compute metrics at a given threshold\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    # Ensure y_true is a numpy array\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "        \n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "    prevalence = (tp + fn) / (tp + tn + fp + fn)\n",
    "    return dict(\n",
    "        threshold=float(thr), tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sensitivity), specificity=float(specificity),\n",
    "        ppv=float(ppv), npv=float(npv), prevalence=float(prevalence)\n",
    "    )\n",
    "\n",
    "# Sweep thresholds on Validation\n",
    "grid = np.unique(np.quantile(y_val_prob, q=np.linspace(0, 1, 501)))\n",
    "val_rows = [metrics_at_threshold(y_val, y_val_prob, thr) for thr in grid]\n",
    "val_tbl = pd.DataFrame(val_rows)\n",
    "\n",
    "# Pick thresholds closest to targets\n",
    "thr_for_sens = val_tbl.iloc[(val_tbl[\"sensitivity\"] - cfg.target_sensitivity).abs().argsort()].iloc[0][\"threshold\"]\n",
    "thr_for_spec = val_tbl.iloc[(val_tbl[\"specificity\"] - cfg.target_specificity).abs().argsort()].iloc[0][\"threshold\"]\n",
    "\n",
    "wandb.config.update({\n",
    "    \"chosen_threshold_sensitivity\": float(thr_for_sens),\n",
    "    \"chosen_threshold_specificity\": float(thr_for_spec)\n",
    "}, allow_val_change=True)\n",
    "\n",
    "# Apply frozen thresholds on Test\n",
    "test_at_sens = metrics_at_threshold(y_test, y_test_prob, thr_for_sens)\n",
    "test_at_spec = metrics_at_threshold(y_test, y_test_prob, thr_for_spec)\n",
    "\n",
    "# Log tables\n",
    "wandb.log({\"validation_threshold_sweep\": wandb.Table(dataframe=val_tbl[[\"threshold\",\"sensitivity\",\"specificity\",\"ppv\",\"npv\",\"prevalence\"]])})\n",
    "test_results_df = pd.DataFrame([\n",
    "    dict(target=\"sensitivity\", **test_at_sens),\n",
    "    dict(target=\"specificity\", **test_at_spec)\n",
    "])\n",
    "wandb.log({\"test_operating_points\": wandb.Table(dataframe=test_results_df)})\n",
    "\n",
    "# --- ENHANCEMENT: Log interactive Confusion Matrices ---\n",
    "y_pred_sens = (y_test_prob >= thr_for_sens).astype(int)\n",
    "y_pred_spec = (y_test_prob >= thr_for_spec).astype(int)\n",
    "wandb.log({\n",
    "    \"confusion_matrix_test_at_sensitivity\": wandb_cm(y_true=y_test.values, preds=y_pred_sens, class_names=[\"Survived\", \"Died\"]),\n",
    "    \"confusion_matrix_test_at_specificity\": wandb_cm(y_true=y_test.values, preds=y_pred_spec, class_names=[\"Survived\", \"Died\"])\n",
    "})\n",
    "print(f\"Chosen thresholds -> Sensitivity target: {thr_for_sens:.3f}, Specificity target: {thr_for_spec:.3f}\")\n",
    "\n",
    "# --- 5. Subgroup Performance (from original cell 26) ---\n",
    "SOFA_COL = \"SOFA\"\n",
    "CSRU_COL = \"CSRU\"\n",
    "test_idx = X_test.index\n",
    "sofa_test = ICU.loc[test_idx, SOFA_COL]\n",
    "csru_test = ICU.loc[test_idx, CSRU_COL]\n",
    "sofa_bins = pd.qcut(sofa_test, q=5, duplicates=\"drop\").astype(str)\n",
    "csru_group = np.where(pd.to_numeric(csru_test, errors=\"coerce\").fillna(0).astype(int) == 1, \"CSRU\", \"non_CSRU\")\n",
    "\n",
    "# Helper (renamed to avoid conflict)\n",
    "def subgroup_metrics_fixed_threshold(y_true, y_prob, thr):\n",
    "    # Ensure y_true is a numpy array\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "        \n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    spec = tn / (tn + fp) if (tn + fp) else np.nan\n",
    "    ppv  = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "    npv  = tn / (tn + fn) if (tn + fn) else np.nan\n",
    "    prev = (tp + fn) / (tp + tn + fp + fn)\n",
    "    return dict(\n",
    "        tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sens), specificity=float(spec),\n",
    "        ppv=float(ppv), npv=float(npv), prevalence=float(prev)\n",
    "    )\n",
    "\n",
    "subgroup_rows = []\n",
    "def add_group(group_name, group_values):\n",
    "    series = pd.Series(group_values, index=test_idx).astype(str)\n",
    "    for g in sorted(series.unique()):\n",
    "        mask = (series == g).values\n",
    "        y_true_g = y_test.values[mask]\n",
    "        y_prob_g = y_test_prob[mask]\n",
    "        if len(y_true_g) < 10: continue\n",
    "            \n",
    "        # Ensure y_true_g is a numpy array for metrics\n",
    "        if isinstance(y_true_g, pd.Series):\n",
    "            y_true_g = y_true_g.values\n",
    "            \n",
    "        auroc = roc_auc_score(y_true_g, y_prob_g)\n",
    "        auprc = average_precision_score(y_true_g, y_prob_g)\n",
    "        m_sens = subgroup_metrics_fixed_threshold(y_true_g, y_prob_g, thr_for_sens)\n",
    "        m_spec = subgroup_metrics_fixed_threshold(y_true_g, y_prob_g, thr_for_spec)\n",
    "        subgroup_rows.append({\"subgroup_type\": group_name, \"subgroup_value\": g, \"n\": int(len(y_true_g)), \"auroc\": float(auroc), \"auprc\": float(auprc), \"target\": \"sensitivity\", \"threshold\": float(thr_for_sens), **m_sens})\n",
    "        subgroup_rows.append({\"subgroup_type\": group_name, \"subgroup_value\": g, \"n\": int(len(y_true_g)), \"auroc\": float(auroc), \"auprc\": float(auprc), \"target\": \"specificity\", \"threshold\": float(thr_for_spec), **m_spec})\n",
    "\n",
    "add_group(\"SOFA_bin\", sofa_bins)\n",
    "add_group(\"ICU_unit\", csru_group)\n",
    "subgroup_df = pd.DataFrame(subgroup_rows)\n",
    "wandb.log({\"subgroup_metrics_test\": wandb.Table(dataframe=subgroup_df)})\n",
    "print(\"Logged subgroup metrics for SOFA bins and CSRU\")\n",
    "\n",
    "# --- 6. Finish Comprehensive Run ---\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f4af3",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Go to the W&B project page and compare the three `baseline` runs (LR, DT, RF)\n",
    "- Add `val_auc`, `val_pr`, and `val_brier` to the comparison table\n",
    "- Open the \"02-baseline-random-forest-full\" run. This run contains everything:\n",
    "    - **Metrics**: Check `val_pr` and `test_pr`\n",
    "    - **Plots**: Review the `roc_curve_val` and `calibration_table_val` (create a plot in the UI)\n",
    "    - **Thresholds**: Inspect the `test_operating_points` table and the `confusion_matrix...` plots\n",
    "    - **Fairness**: Analyze the `subgroup_metrics_test` table. How does `auroc` or `sensitivity` change between SOFA bins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195184a",
   "metadata": {},
   "source": [
    "# 8. Interpretability for understanding\n",
    "\n",
    "Model interpretability connects predictive performance to clinical meaning:\n",
    "- Use **Permutation Importance** for Logistic Regression, Decision Tree, and Random Forest  \n",
    "- Use **SHAP** on the Random Forest to see which features drive individual predictions  \n",
    "\n",
    "All outputs are logged to Weights & Biases for exploration and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability: Permutation Importance and SHAP for Random Forest\n",
    "# Logs all interpretability outputs to W&B\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"interpretability\",\n",
    "    name=\"03-interpretability-report\", # Add clean name\n",
    "    config={\n",
    "        \"models\": [\"logistic_regression\", \"decision_tree\", \"random_forest\"],\n",
    "        \"shap_sample_size\": 500\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# 1) Permutation Importance for all models on Validation\n",
    "models = {\n",
    "    \"logistic_regression\": log_reg,\n",
    "    \"decision_tree\": dt,\n",
    "    \"random_forest\": rf\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    result = permutation_importance(\n",
    "        model, X_val_t, y_val, n_repeats=10, random_state=SEED, n_jobs=-1\n",
    "    )\n",
    "    imp_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X_val_t.columns,\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std\n",
    "        })\n",
    "        .sort_values(\"importance_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    wandb.log({f\"{model_name}_permutation_importance\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# 2) SHAP for Random Forest\n",
    "shap_sample_n = min(wandb.config.shap_sample_size, len(X_val_t))\n",
    "shap_sample = X_val_t.sample(n=shap_sample_n, random_state=SEED)\n",
    "\n",
    "explainer = shap.TreeExplainer(\n",
    "    rf,\n",
    "    model_output=\"raw\",\n",
    "    feature_perturbation=\"tree_path_dependent\"\n",
    ")\n",
    "\n",
    "# This complex logic handles inconsistencies in shap/sklearn versions\n",
    "shap_values_raw = explainer.shap_values(shap_sample, check_additivity=False)\n",
    "if isinstance(shap_values_raw, list):\n",
    "    sv = np.asarray(shap_values_raw[1])\n",
    "else:\n",
    "    sv = np.asarray(shap_values_raw)\n",
    "if sv.ndim == 3:\n",
    "    cls_axis = sv.shape[-1]\n",
    "    pick = 1 if cls_axis >= 2 else 0\n",
    "    sv = sv[..., pick]\n",
    "sv = np.squeeze(sv)\n",
    "assert sv.ndim == 2, f\"Expected 2D SHAP values, got {sv.shape}\"\n",
    "assert sv.shape[1] == shap_sample.shape[1], \"Feature count mismatch\"\n",
    "\n",
    "# Global summary plot\n",
    "shap.summary_plot(sv, shap_sample, show=False)\n",
    "wandb.log({\"shap_summary_plot\": wandb.Image(plt.gcf())})\n",
    "plt.close()\n",
    "\n",
    "# Ranked mean absolute SHAP for table\n",
    "mean_abs_shap = np.abs(sv).mean(axis=0).reshape(-1)\n",
    "feat_names = list(shap_sample.columns)\n",
    "shap_df = pd.DataFrame(\n",
    "    {\"feature\": feat_names, \"mean_abs_shap\": mean_abs_shap}\n",
    ").sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "wandb.log({\"rf_shap_feature_importance\": wandb.Table(dataframe=shap_df)})\n",
    "\n",
    "# --- ENHANCEMENT: Log SHAP Dependence Plots for top 2 features ---\n",
    "print(\"Logging SHAP dependence plots for top 2 features...\")\n",
    "top_features = shap_df[\"feature\"].head(2).tolist()\n",
    "\n",
    "for feature_name in top_features:\n",
    "    try:\n",
    "        fig, ax = plt.subplots()\n",
    "        shap.dependence_plot(feature_name, sv, shap_sample, ax=ax, show=False, interaction_index=None)\n",
    "        wandb.log({f\"shap_dependence_{feature_name}\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot dependence for {feature_name}: {e}\")\n",
    "        plt.close('all') # Close any open figures to avoid bleed-over\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cf119",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- In the `03-interpretability-report` run, compare the permutation importance tables across the three models\n",
    "- Examine the `shap_summary_plot` to see which features drive RF predictions (e.g., SOFA, CSRU)\n",
    "- Review the `shap_dependence_...` plots to understand *how* top features impact mortality risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d902532",
   "metadata": {},
   "source": [
    "# 9. Random Forest hyperparameter sweep\n",
    "\n",
    "We will run a short Weights & Biases Sweep to tune Random Forest hyperparameters with the goal if **Maximizing validation area under the Precision-Recall curve** \n",
    "- REmmeber Precision-Recall is more informative than ROC under class imbalance\n",
    "\n",
    "We log validation AUROC and Brier score as secondary signals for discrimination and calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest sweep optimized for imbalanced data using validation AUPRC\n",
    "\n",
    "# Sweep training function\n",
    "def train_rf_sweep():\n",
    "    # Use job_type to group sweep agents\n",
    "    run = wandb.init(project=WB_PROJECT, job_type=\"sweep-agent\", reinit=True)\n",
    "    cfg = wandb.config\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=cfg.n_estimators,\n",
    "        max_depth=None if cfg.max_depth == 0 else cfg.max_depth,\n",
    "        max_features=cfg.max_features,\n",
    "        min_samples_leaf=cfg.min_samples_leaf,\n",
    "        class_weight=\"balanced\",   # encourage attention to minority class\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_t, y_train)\n",
    "\n",
    "    # Validation probabilities\n",
    "    y_val_prob = model.predict_proba(X_val_t)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    val_pr   = average_precision_score(y_val, y_val_prob)     # sweep objective\n",
    "    val_auc  = roc_auc_score(y_val, y_val_prob)\n",
    "    val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        \"val_pr\": val_pr,\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_brier\": val_brier\n",
    "    })\n",
    "    run.finish()\n",
    "\n",
    "# Compact search space\n",
    "sweep_config = {\n",
    "    \"name\": \"rf_pr_tuning_v2\", # Give a new name\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_pr\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"n_estimators\": {\"values\": [100, 200, 300, 500]},\n",
    "        \"max_depth\": {\"values\": [0, 8, 12, 16]},          # 0 means None\n",
    "        \"max_features\": {\"values\": [\"sqrt\", \"log2\"]},\n",
    "        \"min_samples_leaf\": {\"values\": [1, 3, 5, 10]}\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Launch the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=WB_PROJECT)\n",
    "print(f\"Sweep started. ID: {sweep_id}\")\n",
    "\n",
    "# Run 5 agents for the demo\n",
    "wandb.agent(sweep_id, function=train_rf_sweep, count=5)\n",
    "\n",
    "print(\"Sweep complete. In Weights & Biases, sort by val_pr and inspect Parameter Importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd112b1d",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Open the W&B Sweep link (e.g., `rf_pr_tuning_v2`)\n",
    "- View the Parallel Coordinates and Parameter Importance plots to see which hyperparameters matter most\n",
    "- Sort the sweep table by `val_pr` (descending) to find the best run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b646d44",
   "metadata": {},
   "source": [
    "# 10. Best model selection, Test evaluation, and Model Registry\n",
    "\n",
    "Let's pick the Random Forest configuration with the best validation Precision-Recall area from the sweep.\n",
    "- We will use the **W&B API** to programmatically fetch the best run's config\n",
    "- We refit that model on Train + Validation\n",
    "- We evaluate on Test\n",
    "- We log the final model as a **W&B Artifact** and register it in the **Model Registry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00aae77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing W&B API to find best sweep run...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\idiaz\\OneDrive - IE University\\00. IE Courses\\01. 2025_H2\\1. ML4HC\\ICU_Survival_analysis\\wandb\\run-20251023_104244-k0qjf2te</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/k0qjf2te' target=\"_blank\">dainty-donkey-48</a></strong> to <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/k0qjf2te' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/k0qjf2te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-donkey-48</strong> at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/k0qjf2te' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/k0qjf2te</a><br> View project at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251023_104244-k0qjf2te\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching sweep data. Using fallback parameters. Error: name 'sweep_id' is not defined\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\idiaz\\OneDrive - IE University\\00. IE Courses\\01. 2025_H2\\1. ML4HC\\ICU_Survival_analysis\\wandb\\run-20251023_104247-v6p0tpgp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/v6p0tpgp' target=\"_blank\">04-final-model-fallback</a></strong> to <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/v6p0tpgp' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/v6p0tpgp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining best model on Train + Validation data...\n",
      "Final Test AUROC 0.981, AUPRC 0.892, Brier 0.057, ECE 0.101\n",
      "Logging model to W&B Artifacts and Model Registry...\n",
      "Successfully logged and registered model artifact\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_brier</td><td>▁</td></tr><tr><td>test_ece_final</td><td>▁</td></tr><tr><td>test_pr</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>0.98138</td></tr><tr><td>test_brier</td><td>0.05711</td></tr><tr><td>test_ece_final</td><td>0.1015</td></tr><tr><td>test_pr</td><td>0.89155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">04-final-model-fallback</strong> at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro/runs/v6p0tpgp' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro/runs/v6p0tpgp</a><br> View project at: <a href='https://wandb.ai/idiazl/ml-healthcare-intro' target=\"_blank\">https://wandb.ai/idiazl/ml-healthcare-intro</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251023_104247-v6p0tpgp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final Random Forest evaluation using the best sweep config\n",
    "\n",
    "# --- 1. Fetch Best Run from Sweep ---\n",
    "print(\"Initializing W&B API to find best sweep run...\")\n",
    "api = WandbApi()\n",
    "\n",
    "# We need the full sweep path: f\"{entity}/{project}/{sweep_id}\"\n",
    "# We can get the entity by starting a temporary run\n",
    "try:\n",
    "    temp_run = wandb.init(project=WB_PROJECT, job_type=\"api_helper\", reinit=True)\n",
    "    ENTITY = temp_run.entity\n",
    "    temp_run.finish()\n",
    "    \n",
    "    sweep_path = f\"{ENTITY}/{WB_PROJECT}/{sweep_id}\"\n",
    "    print(f\"Accessing sweep at: {sweep_path}\")\n",
    "    sweep = api.sweep(sweep_path)\n",
    "    \n",
    "    best_run = sweep.best_run()\n",
    "    print(f\"Found best run: {best_run.name} with val_pr: {best_run.summary['val_pr']:.4f}\")\n",
    "\n",
    "    # --- 2. Get Best Parameters ---\n",
    "    best_params_config = best_run.config\n",
    "    \n",
    "    # Re-create the logic from the sweep function (e.g., max_depth=0 -> None)\n",
    "    rf_params = {\n",
    "        \"n_estimators\": best_params_config[\"n_estimators\"],\n",
    "        \"max_depth\": None if best_params_config[\"max_depth\"] == 0 else best_params_config[\"max_depth\"],\n",
    "        \"max_features\": best_params_config[\"max_features\"],\n",
    "        \"min_samples_leaf\": best_params_config[\"min_samples_leaf\"],\n",
    "        \"class_weight\": \"balanced\" # This was fixed in our sweep\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching sweep data. Using fallback parameters. Error: {e}\")\n",
    "    # Fallback in case API fails in a restricted environment\n",
    "    rf_params = {\n",
    "        \"n_estimators\": 300, \"max_depth\": 12, \"max_features\": \"sqrt\",\n",
    "        \"min_samples_leaf\": 3, \"class_weight\": \"balanced\"\n",
    "    }\n",
    "    best_run = None # Flag that we used fallback\n",
    "\n",
    "# --- 3. Start Final Evaluation Run ---\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"final_eval\",\n",
    "    name=f\"04-final-model-{'fallback' if best_run is None else best_run.name}\",\n",
    "    config=rf_params, # Log the actual params used\n",
    "    reinit=True,\n",
    ")\n",
    "if best_run:\n",
    "    wandb.config.update({\"source_sweep\": sweep_path, \"source_run_id\": best_run.id})\n",
    "\n",
    "# --- Helper function for calibration (needed for final test eval) ---\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    agg = dfb.groupby(\"bin\", observed=False).agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),\n",
    "        observed_rate=(\"y_true\", \"mean\"),\n",
    "        count=(\"y_true\", \"size\")\n",
    "    ).reset_index()\n",
    "    agg[\"bin\"] = agg[\"bin\"].astype(str)\n",
    "    weights = agg[\"count\"] / agg[\"count\"].sum()\n",
    "    ece = float(np.sum(weights * np.abs(agg[\"observed_rate\"] - agg[\"mean_prob\"])))\n",
    "    return agg, ece\n",
    "# --- End Helper ---\n",
    "\n",
    "# --- 4. Retrain Model on Train+Val ---\n",
    "print(\"Retraining best model on Train + Validation data...\")\n",
    "X_train_full_t = pd.concat([X_train_t, X_val_t])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "rf_best = RandomForestClassifier(\n",
    "    **rf_params, # Unpack the fetched params\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_best.fit(X_train_full_t, y_train_full)\n",
    "\n",
    "# --- 5. Evaluate and Log on Test ---\n",
    "y_test_prob = rf_best.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "test_pr = average_precision_score(y_test, y_test_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# --- ENHANCEMENT: Log Final Test Curves ---\n",
    "y_test_probas_2d = np.stack([1.0 - y_test_prob, y_test_prob], axis=1)\n",
    "wandb.log({\n",
    "    \"roc_curve_test_final\": wandb_roc_curve(y_test.values, y_test_probas_2d),\n",
    "    \"pr_curve_test_final\": wandb_pr_curve(y_test.values, y_test_probas_2d)\n",
    "})\n",
    "\n",
    "# Log calibration table and predictions\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\n",
    "    \"test_ece_final\": test_ece,\n",
    "    \"calibration_table_test_final\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "pred_tbl = pd.DataFrame({\"id\": X_test.index, \"y_true\": y_test.values, \"y_prob\": y_test_prob})\n",
    "wandb.log({\"final_test_predictions\": wandb.Table(dataframe=pred_tbl)})\n",
    "\n",
    "print(f\"Final Test AUROC {test_auc:.3f}, AUPRC {test_pr:.3f}, Brier {test_brier:.3f}, ECE {test_ece:.3f}\")\n",
    "\n",
    "# --- 6. Log Model as Artifact and Register ---\n",
    "print(\"Logging model to W&B Artifacts and Model Registry...\")\n",
    "model_file = \"final_rf_model.joblib\"\n",
    "joblib.dump(rf_best, model_file)\n",
    "\n",
    "# Define the artifact\n",
    "model_at = wandb.Artifact(\n",
    "    \"best-rf-model\",\n",
    "    type=\"model\",\n",
    "    description=\"Final Random Forest model trained on train+val, tuned for AUPRC.\",\n",
    "    metadata=rf_params\n",
    ")\n",
    "model_at.add_file(model_file)\n",
    "\n",
    "# Log the artifact to the run\n",
    "run.log_artifact(model_at, aliases=[\"production_candidate\"])\n",
    "\n",
    "# Register the model in the Model Registry\n",
    "try:\n",
    "    run.link_artifact(model_at, f\"{WB_PROJECT}/ICU_Mortality_RF_Model\")\n",
    "    print(\"Successfully logged and registered model artifact\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not auto-register model. Logged artifact instead. Error: {e}\")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536e19b",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Open the `04-final-eval` run\n",
    "- Compare its `test_pr` metric against the `02-baseline-random-forest-full` run to quantify the impact of tuning\n",
    "- Go to the **Artifacts** tab in the W&B project (left-hand sidebar)\n",
    "- Find the `best-rf-model` artifact and inspect its contents and metadata\n",
    "- Go to the **Models** tab to see the registered `ICU_Mortality_RF_Model` and its version history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8c091",
   "metadata": {},
   "source": [
    "# 11. Final Documentation: Model Card and Reporting\n",
    "\n",
    "The final step in a responsible ML workflow is documentation. This ensures that the model's performance, limitations, and intended use are understood by all stakeholders (e.g., clinicians, regulators, engineers).\n",
    "\n",
    "In Weights & Biases, you can create a **W&B Report** to weave together your findings into a comprehensive model card.\n",
    "\n",
    "### A good report for this project would include:\n",
    "\n",
    "-   **Objective**: The clinical goal (predicting in-hospital mortality)\n",
    "-   **Data**: A link to the `01-data-exploration` run, showing class balance and missingness\n",
    "-   **Models**: The \"Model Comparison\" table (from comparing `baseline` runs) showing why Random Forest was chosen\n",
    "-   **Tuning**: Key visualizations from the `rf_pr_tuning_v2` sweep\n",
    "-   **Final Performance**: Final metrics from the `04-final-eval` run (Test AUROC, AUPRC, Brier)\n",
    "-   **Interpretability**: The `shap_summary_plot` and `shap_dependence_...` plots from the `03-interpretability-report` run\n",
    "-   **Fairness & Safety**: The `subgroup_metrics_test` table, discussing performance on SOFA and CSRU groups\n",
    "-   **Clinical Use**: The `test_operating_points` table, explaining the trade-offs between the sensitivity and specificity thresholds\n",
    "\n",
    "You can also attach this information directly to the registered model in the W&B Model Registry UI. This creates a \"model card\" that lives with the model, ensuring downstream users understand its strengths and limitations before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664a395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd598a80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce20097c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu-survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
